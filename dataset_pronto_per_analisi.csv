DOI,Abstract,Index Keywords,Source title,Year
https://doi.org/10.1177/10776958221149577,Generative artificial intelligence (AI) is ushering in an era of potential transformation of journalism and media content. This essay considers one notable generative AI platform called ChatGPT made available to the public in 2022 for free use. ChatGPT allows users to enter text prompts and rapidly generates text responses drawn from its knowledge acquired via machine learning in engagement with the internet. This essay is coauthored by a human journalism and media professor in collaboration with ChatGPT. The essay demonstrates the capacity and limitations of ChatGPT and offers reflections on the implications of generative AI for journalism and media education.,Generative model;Technical Journalism,Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education,2023
https://doi.org/10.1126/science.adh2586,"We examined the productivity effects of a generative artificial intelligence (AI) technology, the assistive chatbot ChatGPT, in the context of midlevel professional writing tasks. In a preregistered online experiment, we assigned occupation-specific, incentivized writing tasks to 453 college-educated professionals and randomly exposed half of them to ChatGPT. Our results show that ChatGPT substantially raised productivity: The average time taken decreased by 40% and output quality rose by 18%. Inequality between workers decreased, and concern and excitement about AI temporarily rose. Workers exposed to ChatGPT during the experiment were 2 times as likely to report using it in their real job 2 weeks after the experiment and 1.6 times as likely 2 months after the experiment.",Chatbot,Experimental evidence on the productivity effects of generative artificial intelligence,2023
https://doi.org/10.1016/j.ijinfomgt.2023.102716,"While all functional areas in organizations are benefiting from the recent development in generative artificial intelligence (GAI), marketing has been particularly affected positively by this breakthrough innovation. However, scholars have not paid attention to the transformative impacts GAI has on marketing activities. This editorial article aims to fill this void. It outlines the current state of generative artificial intelligence in marketing. The article discusses the facilitators and barriers for the use of generative artificial intelligence in marketing. It highlights the effectiveness of insights generated by GAI in personalizing content and offerings and argues that marketing content generated by GAI is likely to be more personally relevant than that produced by earlier generations of digital technologies. The article explains how higher efficiency and productivity of marketing activities can be achieved by using GAI to create marketing content. It also describes the roles of insights and marketing content generated by GAI to improve the sales lead generation process. Implications for research, practice and policy are also discussed.",Transformative Learning,"Generative artificial intelligence in marketing: Applications, opportunities, challenges, and research agenda",2023
https://doi.org/10.1007/s40979-024-00149-4,"Abstract The aim of this project was to understand student perspectives on generative artificial intelligence (GAI) technologies such as Chat generative Pre-Trained Transformer (ChatGPT), in order to inform changes to the University of Liverpool Academic Integrity code of practice. The survey for this study was created by a library student team and vetted through focus groups. A total of 2555 students participated in the survey. Results showed that only 7% of students who responded had not heard of any GAI technologies, whilst over half had used or considered using these for academic purposes. The majority of students (54.1%) were supportive or somewhat supportive of using tools such as Grammarly, but 70.4% were unsupportive or somewhat unsupportive towards students using tools such as ChatGPT to write their whole essay. Students who had higher levels of confidence in their academic writing were less likely to use or consider using them for academic purposes, and were also less likely to be supportive of other students using them. Most students (41.1%) also thought there should be a university wide policy on when these technologies are or are not appropriate to use. The results of this research suggest that students require clear policies on the use of GAI and that these technologies should not be banned from university, but consideration must be made to ensure different groups of students have equal access to the technologies.",Generative grammar;Mathematics education;Artificial intelligence;Computer science;Psychology,Student perspectives on the use of generative artificial intelligence technologies in higher education,2024
https://doi.org/10.1145/3531146.3533077,"Artificial Intelligence (AI) can generate text virtually indistinguishable from text written by humans. A key question, then, is whether people believe news headlines generated by AI as much as news headlines generated by humans. AI is viewed as lacking human motives and emotions, suggesting that people might view news written by AI as more accurate. By contrast, two pre-registered experiments on representative U.S. samples (N = 4,034) showed that people rated news headlines written by AI as less accurate than those written by humans. People were more likely to incorrectly rate news headlines written by AI (vs. a human) as inaccurate when they were actually true, and more likely to correctly rate them as inaccurate when they were indeed false. Our findings are important given the increasing adoption of AI in news generation, and the associated ethical and governance pressures to disclose it use and address standards of transparency and accountability.",Transparency (behavior);Generative grammar;Accountability;Artificial intelligence;Computer science;Key (lock);Psychology;Political science;Computer security;Law,News from Generative Artificial Intelligence Is Believed Less,2022
https://doi.org/10.1093/pnasnexus/pgae035,"The increasing availability of microtargeted advertising and the accessibility of generative artificial intelligence (AI) tools, such as ChatGPT, have raised concerns about the potential misuse of large language models in scaling microtargeting efforts for political purposes. Recent technological advancements, involving generative AI and personality inference from consumed text, can potentially create a highly scalable ""manipulation machine"" that targets individuals based on their unique vulnerabilities without requiring human input. This paper presents four studies examining the effectiveness of this putative ""manipulation machine."" The results demonstrate that personalized political ads tailored to individuals' personalities are more effective than nonpersonalized ads (studies 1a and 1b). Additionally, we showcase the feasibility of automatically generating and validating these personalized ads on a large scale (studies 2a and 2b). These findings highlight the potential risks of utilizing AI and microtargeting to craft political messages that resonate with individuals based on their personality traits. This should be an area of concern to ethicists and policy makers.",Generative grammar;Artificial intelligence;Politics;Psychology;Computer science;Political science;Law,The persuasive effects of political microtargeting in the age of generative artificial intelligence,2024
https://doi.org/10.17705/1jais.00859,"In this editorial, revisiting Alavi and Leidner (2001) as a conceptual lens, we consider the organizational implications of Generative Artificial Intelligence (GenAI) from a knowledge management (KM) perspective. We examine how GenAI impact the processes of knowledge creation, storage, transfer, and application, highlighting both the opportunities and challenges this technology presents. In knowledge creation, GenAI enhances information? processing and cognitive functions, fostering individual and organizational learning. However, it also introduces risks like AI bias and reduced human socialization, potentially marginalizing junior knowledge workers. For knowledge storage and retrieval, GenAI’s ability to quickly access vast knowledge bases significantly changes employee interactions with KM systems. This raises questions about balancing human-derived tacit knowledge with AI-generated explicit knowledge. The paper also explores GenAI’s role in knowledge transfer, particularly in training and cultivating a learning culture. Challenges include an over-reliance on AI and risks in disseminating sensitive information. In terms of knowledge application, GenAI is seen as a tool to boost productivity and innovation, but issues like knowledge misapplication, intellectual property, and ethical considerations are critical. Conclusively, the paper argues for a balanced approach to integrating GenAI into KM processes. It advocates for harmonizing GenAI’s capabilities with human insights to effectively manage knowledge in contemporary organizations, ensuring both technological advances and ethical responsibility.",Tacit Knowledge;Explicit knowledge,Knowledge Management Perspective of Generative Artificial Intelligence,2024
https://doi.org/10.3145/epi.2023.mar.27,"The implementation of artificial intelligence techniques and tools in the media will systematically and continuously alter their work and that of their professionals during the coming decades. To this end, this article carries out a systematic review of the research conducted on the implementation of AI in the media over the last two decades, particularly empirical research, to identify the main social and epistemological challenges posed by its adoption. For the media, increased dependence on technological platforms and the defense of their editorial independence will be the main challenges. Journalists, in turn, are torn between the perceived threat to their jobs and the loss of their symbolic capital as intermediaries between reality and audiences, and a liberation from routine tasks that subsequently allows them to produce higher quality content. Meanwhile, audiences do not seem to perceive a great difference in the quality and credibility of automated texts, although the ease with which texts are read still favors human authorship. In short, beyond technocentric or deterministic approaches, the use of AI in a specifically human field such as journalism requires a social approach in which the appropriation of innovations by audiences and the impact it has on them is one of the keys to its development. Therefore, the study of AI in the media should focus on analyzing how it can affect individuals and journalists, how it can be used for the proper purposes of the profession and social good, and how to close the gaps that its use can cause.",Appropriation;Technical Journalism,"Without journalists, there is no journalism: the social dimension of generative artificial intelligence in the media",2023
https://doi.org/10.1080/10382046.2023.2194036,"Much discussion has been surrounding generative AI language models, such as ChatGPT, and the advantages and threats they present to education. As an AI language model, ChatGPT has the potential to transform geography and environmental education. The extensive knowledge base and natural language processing abilities that ChatGPT possesses make it a perfect tool for this purpose; it can actively include students in conversation (as the chat in its name implies) while also providing them with quick feedback. Because of this, students are free to progress through the material at their own pace, using strategies that best suit them individually. Yet, this change has been criticised by others who worry about intellectual property violations and undermining academic integrity. As discussions continue about whether generative AI models are a boon or bane for education, we would like to focus on the problems and potential facing geography and environmental education in particular.

We did a simple experiment and asked ChatGPT to: ""Write a short paragraph to explain how geographical education can support a child’s development cognitively and affectively, in responding to the environmental challenges of our times.""

This was the result.

Geographical education plays a critical role in a child’s cognitive and affective development, particularly in addressing the environmental challenges of our times. By learning about the physical and social geography of their local and global communities, children can develop a greater awareness and understanding of the interconnectedness of natural and human systems. This knowledge can help them to develop critical thinking skills, problem-solving abilities, and a sense of responsibility towards the environment. Moreover, geographical education can foster empathy and respect for cultural diversity, as well as an appreciation for the beauty and complexity of the world we inhabit. Overall, geographical education is an essential tool for equipping children with the skills, knowledge, and attitudes necessary to respond effectively to environmental challenges and to become responsible global citizens.

This response was generated at 0050h GMT on 9 March 2023. We are keenly aware that as ChatGPT continues to learn from user input and revisions, the answer will likely change when you try it at a different time.

The generated paragraph covers the key points of geographical knowledge, skills and attitudes and summarises the points in the last sentence of how these developments will help a child appreciate their environment better. It is a safe answer, and there is nothing wrong with that. However, it is also lacklustre and does not say anything beyond what most educators in geographical and environmental education already know.

Nonetheless, it provided an instant response, was grammatically correct and gave a starting point from which the paragraph could be improved and possibly even allow us to reflect on our own versions or writing styles.

Perhaps the question to ask is whether students will use this tool and add value to their geographical and environmental education, or instead how do we link the knowledge, skills and attitudes to behavioural change. One quick answer that comes to mind is that it can help students better grasp environmental challenges and how they connect to geography by referring to real-world settings. For instance, this can lead to better-informed decisions and an improved understanding of environmental issues. We asked ChatGPT what Tsunamis are, and it gave a relatively standard textbook answer. We then asked if I needed to worry about Tsunamis if I lived in Singapore. Here is the extracted answer at 0100h GMT on 9 March 2023

While Singapore is not as prone to tsunamis as some other coastal regions, it is still possible for a tsunami to affect the country … Moreover, Singapore is a low-lying island city-state, and many of its densely populated areas are located near the coast. In the event of a tsunami, these areas could be at risk of flooding and damage…. However, it is important to note that the risk of a tsunami affecting Singapore is relatively low… Overall, while the risk of a tsunami affecting Singapore is not zero, it is important to remain vigilant and informed about the potential risks, and to follow the guidance and instructions of local authorities in the event of an emergency.

Generative AI can provide a contextualised response to the real-world context. It even goes as far as to offer advice on what to do when there is a tsunami warning. While Singapore is generally safe from tectonics related tsunami, there is still some chance that a tsunami of wave height that could be potentially dangerous can occur for this island state. That said, the use of generative AI is useful for rapid learning but also poses some risks to geographical and environmental education. One of the most significant challenges is the risk of pupils relying too heavily on AI-generated material and failing to acquire critical thinking skills. With ChatGPT offering answers to all sorts of queries, it is crucial to get students to challenge the information they are provided and to think critically about the sources of information. It is also equally important for students to know how to ask the questions to the generative AI model so that they can get relevant and meaningful answers. In geographical and environmental education, we need to identify the sorts of criticality that we want our children to have.

In previous editorials, we have asserted that geographical education should consider the following:

What geography do students need to know? What are student misconceptions and preconceptions? How can we enhance students’ geographical knowledge, understanding and skills?

How do we understand students’ learning progressions in geography? How are these best calibrated and assessed?

What are the characteristics of effective teaching and learning materials and resources in improving the quality of geographical education?

What teaching methods are efficient and effective in improving the quality of geographical education?

How can the education of geography teachers be improved to raise the quality of teaching and the levels of achievement in geography in schools?

(Commission on Geographical Education, Citation2016 p. 6)

In answering the first question, generative AI, for example, teachers can make personalised simulations and virtual settings that make it easier and more enjoyable for students to learn about geographical phenomena enhanced knowledge and skills. This method can help students better understand difficult topics that are hard to picture independently. But the use of generative AI for the subsequent questions in the list above becomes less obvious. We argue that while generative AI can be used as supporting tools to answer these questions, there must be some way to guide how students use the tools. In the two examples of how we have used ChatGPT in the sections above, you will notice that the phrased questions are done so deliberately to solicit a certain scope of answers. Hence, the quality of the answer is only as good as the quality of the question posed to the generative AI.

Another thing that could go wrong with generative AI models like ChatGPT is that the amount and quality of human interaction in the classroom could go down. AI can make learning more personalised, but it can’t replace the importance of face-to-face interactions between students and teachers and between students. Because of this, it is essential to use ChatGPT to add to and not replace other ways of teaching. There are still some things that ChatGPT cannot do for geographical and environmental education. These include innovative pedagogies, especially for fieldwork, and learning beyond textual, visual and auditory modes, such as using Geographic Information systems (GIS). Indeed, knowing what teaching methods are efficient and effective in improving the quality of geographical education is one of the key questions among geography educators, as expounded in the International Charter on Geographical Education (Commission on Geographical Education, Citation2016). The operative word is yet. With the advancement in technology and AI models continuing to “learn,” these may be possible soon too.

The articles featured in this issue concern student learning to curriculum and pedagogy in geographical and environmental education. We have a paper on the influence of Environmental Education on Omani students’ self-reported environmental attitudes and behaviours, with a focus on what students learn. Another paper focuses on the difficulties of distance geographical education in Poland. We have a paper on how flood risk is explained in Spanish school geography for pedagogy. There are also two papers in this issue on the use of GIS. The variety of topics is typical of the issues discussed in this journal (refer to Kidman & Papadimitriou, Citation2012 where the key themes of the journal were identified), but the current discussion on AI raises the question of whether the pace of geographical and environmental education research is keeping up with these rapid changes. The editors encourage researchers to submit papers in this area so that we can start discussing the key issues and discover better ways to engage these emerging technologies.

However, we need a framework to consider how best we can use AI tools like ChatGPT to support good and meaningful geographical and environmental education. We need an angle for this piece. Otherwise, this editorial will probably fall into the same level of mediocrity as one generated by ChatGPT. Chang (Citation2023) has argued that for learning a topic like climate change within the geography curriculum, a framework like the UNESCO four pillars of Education for the 21st century (Delors, Citation1998) can be used. Delors (Citation1998, p. 97) advocates for an integrated approach to education based on:

learning to know

learning to do

learning to be

learning to live together

Generative AI like ChatGPT can help people learn by giving them ready and easy access to information and resources. AI-powered educational platforms, for example, can be designed to tailor learning paths to a student’s interests, abilities, and learning style and readiness. Generative AI can also be used to help students learn practical skills through simulations and virtual environments for practice and experimentation. While some may believe that learning to be is more challenging to support with Generative AI, when properly designed, a learning activity that encourages reflection, empathy, and self-awareness can help students learn to be, nudge people into self-reflection and motivate personal growth. In addition, Generative AI can help people collaborate, such as by facilitating global interaction between students and teachers from different places, allowing them to learn from each other’s cultural and social context. However, the premise for using AI is in the design of the learning activity, not merely in using the AI platform itself. The key to these four principles is through meaningful design and development of good and effective learning activities, with AI and not just generated by AI.

Education Technology guru Chris Dede (cited in Anderson, Citation2023) is not too worried about the growing concerns about generative AI in education, like ChatGPT. As someone who has studied emerging technologies for a long time, he has seen many instances where new technologies seemed like they would change the field. Instead, Dede says that teachers need to get smarter about how they teach to make the most of what AI offers. We are sure that when the Gutenberg Press was invented, some people were concerned that woodblock printing technology would be displaced. Teachers were also probably concerned that with the rise of the scientific calculator, students will no longer be able to do mental math. But we have simply become skilful at using these tools without being less intelligent or critical. In fact, our knowledge base has expanded immensely and we now have much more sophisticated ways of knowing than before. However, as with previous disruptive innovations, generative AI language models such as ChatGPT will bring both opportunities and challenges to geographical and environmental education. While its ability to provide individualised and interactive learning experiences is a significant advantage, students must still be encouraged to develop critical thinking skills and engage in human interactions. We can use ChatGPT's potential to provide students with a more engaging and effective learning experience by striking a balance between AI and our pedagogy. Teachers should have agency in this situation. Indeed, researchers like Siemens et al. (Citation2022) argue for human and artificial cognition (HAC) collaboration in the way we should approach this issue. We should not try to do what AI can do, but instead, add value to learning by doing what it cannot do and work with the tool to create meaningful learning experiences for our learners. In geographical and environmental education, we can look to how we use signature pedagogies like fieldwork (Seow, Chang, & Irvine, Citation2019) to bridge the HAC collaboration. Perhaps AI can help manage the more textual aspects of the fieldwork preparation, assist in the organisation of data collected in the field, and even suggest ways to present the findings. However, the human agency in the HAC is critical in ensuring that the field learning experience is authentic and meaningful. One key drawback of ChatGPT now is that it is based on historical data and it will tell you quite unequivocally that it cannot provide information for anything after 2021. The point to be made is that the quality of learning with AI depends very much on the human part of the HAC collaboration. We will need to understand more examples, critical frameworks, evaluation studies and so forth in geographical and environmental education. IRGEE invites you to submit articles on this topic.

Chang, Kidman, Lidstone, and Stoltman (Citation2022) state that IRGEE has come a long way from the early 1990s when there was a meteoric rise in concerns over the changing environment and the use of technologies like GIS. Today, the academic discourses in geographical and environmental education reflect contemporary concerns with longer-term sustainability as well as the use of emerging technologies (p. 264). There are challenges to using generative AI in geography education as with any new and emerging technology. It is important that children have equal access to the AI regardless of their socioeconomic background. Additionally, there could be potential biases in the data used to train these AI systems. Overall, the rise of generative AI creates exciting new opportunities for geography and environmental education. Educators can use this technology to create more engaging and personalised learning experiences for students, preparing them for future environmental concerns. However, it is critical to thoroughly evaluate the potential issues and ethical implications of implementing this technology. Considering the opportunities and challenges posed by generative AI, we must ensure that our efforts in geographical and environmental education do not degenerate.",Generative model,The rise of generative artificial intelligence (AI) language models - challenges and opportunities for geographical and environmental education,2023
https://doi.org/10.1007/s42001-024-00250-1,"Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we’ll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI’s nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.",Humanity;Generative grammar;Artificial intelligence;Computer science;Cognitive science;Philosophy;Psychology;Theology,GenAI against humanity: nefarious applications of generative artificial intelligence and large language models,2024
https://doi.org/10.1108/ijchm-08-2023-1271,"Purpose The technology acceptance model (TAM) is a widely used framework explaining why users accept new technologies. Still, its relevance is questioned because of evolving consumer behavior, demographics and technology. Contrary to a research paper or systematic literature review, the purpose of this critical reflection paper is to discuss TAM's relevance and limitations in hospitality and tourism research. Design/methodology/approach This paper uses a critical reflective approach, enabling a comprehensive review and synthesis of recent academic literature on TAM. The critical evaluation encompasses its historical trajectory, evolutionary growth, identified limitations and, more specifically, its relevance in the context of hospitality and tourism research. Findings TAM's limitations within the hospitality and tourism context revolve around its individual-centric perspective, limited scope, static nature, cultural applicability and reliance on self-reported measures. Research limitations/implications To optimize TAM's efficacy, the authors propose several strategic recommendations. These include embedding TAM within the specific context of the industry, delving into TAM-driven artificial intelligence adoption, integrating industry-specific factors, acknowledging cultural nuances and using comprehensive research methods, such as mixed methods approach. It is imperative for researchers to critically assess TAM's suitability for their studies and be open to exploring alternative models or methods that can adeptly navigate the distinctive dynamics of the industry. Originality/value This critical reflection paper prompts a profound exploration of technology adoption within the dynamic hospitality and tourism sector, makes insightful inquiries into TAM's future potential and presents recommendations.",Hospitality;Relevance;Scope (computer science);Technology Acceptance Model;Hospitality management studies,Is it the end of the technology acceptance model in the era of generative artificial intelligence?,2024
https://doi.org/10.1145/3613904.3642114,"Among creative professionals, Generative Artificial Intelligence (GenAI) has sparked excitement over its capabilities and fear over unanticipated consequences. How does GenAI impact User Experience Design (UXD) practice, and are fears warranted? We interviewed 20 UX Designers, with diverse experience and across companies (startups to large enterprises). We probed them to characterize their practices, and sample their attitudes, concerns, and expectations. We found that experienced designers are confident in their originality, creativity, and empathic skills, and find GenAI's role as assistive. They emphasized the unique human factors of ""enjoyment"" and ""agency"", where humans remain the arbiters of ""AI alignment''. However, skill degradation, job replacement, and creativity exhaustion can adversely impact junior designers. We discuss implications for human-GenAI collaboration, specifically copyright and ownership, human creativity and agency, and AI literacy and access. Through the lens of responsible and participatory AI, we contribute a deeper understanding of GenAI fears and opportunities for UXD.",Creativity;Agency (philosophy);Originality;Perception;Psychology;Generative grammar;User experience design;Literacy;Citizen journalism;Knowledge management;Applied psychology;Computer science;Social psychology;Human–computer interaction;Sociology;Pedagogy;Artificial intelligence;World Wide Web;Neuroscience;Social science,User Experience Design Professionals’ Perceptions of Generative Artificial Intelligence,2024
https://doi.org/10.1080/02602938.2024.2335321,"Generative artificial intelligence (AI) has rapidly increased capacity for producing textual, visual and auditory outputs, yet there are ongoing concerns regarding the quality of those outputs. There is an urgent need to develop students' evaluative judgement – the capability to judge the quality of work of self and others – in recognition of this new reality. In this conceptual paper, we describe the intersection between evaluative judgement and generative AI with a view to articulating how assessment practices can help students learn to work productively with generative AI. We propose three foci: (1) developing evaluative judgement of generative AI outputs; (2) developing evaluative judgement of generative AI processes; and (3) generative AI assessment of student evaluative judgements. We argue for developing students' capabilities to identify and calibrate quality of work – uniquely human capabilities at a time of technological acceleration – through existing formative assessment strategies. These approaches circumvent and interrupt students' uncritical usage of generative AI. The relationship between evaluative judgement and generative AI is more than just the application of human judgement to machine outputs. We have a collective responsibility, as educators and learners, to ensure that humans do not relinquish their roles as arbiters of quality.",Judgement,Developing evaluative judgement for a time of generative artificial intelligence,2024
https://doi.org/10.1177/00218863231168974,"In this scoping essay, we discuss the potential for generative artificial intelligence (GAI) to shape the work of organizational change, development, or strategy implementation professionals. Using a case example of a culture change initiative, we illustrate how practitioners can benefit from using GAI tools to augment relevant change activities in planning initiative content and roll-out, mobilizing stakeholders, and monitoring initiative progress. We conclude with a reflection on the limitations of GAI systems and outline directions for future research related to (1) stakeholder responses to GAI, (2) GAI impact on the nature of change work, and (3) GAI value creation for change work and unintended consequences of GAI use.",Generative model;Culture change;Managing change;Value (mathematics);Planned change;Unintended consequences,Augmenting Organizational Change and Strategy Activities: Leveraging Generative Artificial Intelligence,2023
https://doi.org/10.17705/1jais.00860,"The discourse surrounding the societal impacts of generative artificial intelligence (GAI), exemplified by technologies like ChatGPT, often oscillates between extremes: utopian visions of unprecedented productivity and dystopian fears of humanity’s demise. This polarized perspective neglects the nuanced, pragmatic manifestation of GAI. In general, extreme views oversimplify the technology itself or its potential to address societal issues. The authors suggest a more balanced analysis, acknowledging that GAI’s impacts will unfold dynamically over time as diverse implementations interact with human stakeholders and contextual factors. While Big Tech firms dominate GAI’s supply, its demand is expected to evolve through experimentation and use cases. The authors argue that GAI’s societal impact depends on identifiable contingencies, emphasizing three broad factors: the balance between automation and augmentation, the congruence of physical and digital realities, and the retention of human bounded rationality. These contingencies represent trade-offs arising from GAI instantiations, shaped by technological advancements, stakeholder dynamics, and contextual factors, including societal responses and regulations. Predicting long-term societal effects remains challenging due to unforeseeable discontinuities in the technology’s trajectory. The authors anticipate a continuous interplay between GAI initiatives, technological advances, learning experiences, and societal responses, with outcomes depending on the above contingencies.",Bounded rationality;Technological determinism;Futures studies;Vision;Dystopia;Transhumanism,The Societal Impacts of Generative Artificial Intelligence: A Balanced Perspective,2024
https://doi.org/10.2139/ssrn.4614223,"Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications.This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher.The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI's nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.",Humanity;Generative grammar;Artificial intelligence;Computer science;Cognitive science;Epistemology;Philosophy;Psychology;Theology,GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models,2023
https://doi.org/10.1007/s11528-023-00926-x,"The release of Generative Artificial Intelligence (GenAI) tools like ChatGPT has sparked vigorous discussion about its potential effects on education. Depending on what you read, GenAI will have an effect somewhere between solving all of education’s problems and destroying learning and education completely. The claim that a new technology can and will solve all of education’s problems is not new, though history tells us that these so-called technological silver bullets have not produced the predicted outcomes (see Thomas Edison’s claims about the phonograph and moving pictures (1878, 1888) or Norman (n.d.)). Is GenAI different? Will it destroy education as we know it? GenAI is fundamentally different from other technologies of the last 20 + years due to its ability to generate original written work that is virtually indistinguishable from that of human authors. While this capability has disruptive implications for education, it is not likely to destroy it. It may, though, destroy the legitimacy of some long-held educational practices.",Generative grammar;Instructional design;Educational technology;Computer science;Artificial intelligence;Knowledge management;Engineering management;Mathematics education;Multimedia;Engineering;Psychology,Innovation of Instructional Design and Assessment in the Age of Generative Artificial Intelligence,2023
https://doi.org/10.1111/1467-8551.12824,"Abstract As businesses and society navigate the potentials of generative artificial intelligence (GAI), the integration of these technologies introduces unique challenges and opportunities for human resources, requiring a re‐evaluation of human resource management (HRM) frameworks. The existing frameworks may often fall short of capturing the novel attributes, complexities and impacts of GAI on workforce dynamics and organizational operations. This paper proposes a strategic HRM framework, underpinned by the theory of institutional entrepreneurship for sustainable organizations, for integrating GAI within HRM practices to boost operational efficiency, foster innovation and secure a competitive advantage through responsible practices and workforce development. Central to this framework is the alignment with existing business objectives, seizing opportunities, strategic resource assessment and orchestration, re‐institutionalization, realignment and embracing a culture of continuous learning and adaptation. This approach provides a detailed roadmap for organizations to navigate successfully the complexities of a GAI‐enhanced business environment. Additionally, this paper significantly contributes to the theoretical discourse by bridging the gap between HRM and GAI adoption, the proposed framework accounting for GAI–human capital symbiosis, setting the stage for future research to empirically test its applicability, explore its implications on HRM practices and understand its broader economic and societal consequences through diverse multi‐disciplinary and multi‐level research methodologies.",Orchestration,Generative Artificial Intelligence in Business: Towards a Strategic Human Resource Management Framework,2024
https://doi.org/10.1287/orsc.2023.18441,"Generative artificial intelligence (AI) holds the potential to either complement workers by enhancing their productivity or substitute them. We examine the short-term effects of the recently released generative AI models (ChatGPT, DALL-E 2, and Midjourney) on the employment outcomes of freelancers on a large online platform. We find that freelancers in highly affected occupations suffer from the introduction of generative AI, experiencing reductions in both employment and earnings. We find similar effects studying the release of other image-based generative AI models. Exploring the heterogeneity by freelancers’ employment history, we do not find evidence that high-quality service, measured by their past performance and employment, moderates the adverse effects on employment. In fact, we find suggestive evidence that top freelancers are disproportionately affected by AI. These results suggest that generative AI may transform the role of human capital in the organization and reduce overall demand for workers.",Generative grammar;Term (time);Industrial organization;Artificial intelligence;Labour economics;Business;Economics;Computer science;Physics;Quantum mechanics,The Short-Term Effects of Generative Artificial Intelligence on Employment: Evidence from an Online Labor Market,2024
https://doi.org/10.1016/j.heliyon.2024.e24727,"In the digital age, where information is a cornerstone for decision-making, social media's not-so-regulated environment has intensified the prevalence of fake news, with significant implications for both individuals and societies. This study employs a bibliometric analysis of a large corpus of 9678 publications spanning 2013–2022 to scrutinize the evolution of fake news research, identifying leading authors, institutions, and nations. Three thematic clusters emerge: Disinformation in social media, COVID-19-induced infodemics, and techno-scientific advancements in auto-detection. This work introduces three novel contributions: 1) a pioneering mapping of fake news research to Sustainable Development Goals (SDGs), indicating its influence on areas like health (SDG 3), peace (SDG 16), and industry (SDG 9); 2) the utilization of Prominence percentile metrics to discern critical and economically prioritized research areas, such as misinformation and object detection in deep learning; and 3) an evaluation of generative AI's role in the propagation and realism of fake news, raising pressing ethical concerns. These contributions collectively provide a comprehensive overview of the current state and future trajectories of fake news research, offering valuable insights for academia, policymakers, and industry.",Generative grammar;Sustainable development;Engineering ethics;Artificial intelligence;Data science;Political science;Engineering;Psychology;Computer science;Law,"Fake news research trends, linkages to generative artificial intelligence and sustainable development goals",2024
https://doi.org/10.1080/01930826.2024.2292484,"In this article, we provide a brief overview of generative artificial intelligence (GenAI) and large language models (LLMs). We then propose eight critical questions that libraries should ask when exploring this technology and its implications for their communities. We argue that libraries have a unique role in facilitating informed and responsible use of GenAI, as well as safeguarding and promoting the values of access, privacy, and intellectual freedom.",Safeguarding;Generative model;Intellectual freedom,Generative Artificial Intelligence: 8 Critical Questions for Libraries,2024
https://doi.org/10.1080/15265161.2023.2278546,,Personhood;Bioethics,Beyond Personhood: Ethical Paradigms in the Generative Artificial Intelligence Era,2024
https://doi.org/10.1177/23780231241259651,"How can generative artificial intelligence (GAI) be used for sociological research? The author explores applications to the study of text and images across multiple domains, including computational, qualitative, and experimental research. Drawing upon recent research and stylized experiments with DALL·E and GPT-4, the author illustrates the potential applications of text-to-text, image-to-text, and text-to-image models for sociological research. Across these areas, GAI can make advanced computational methods more efficient, flexible, and accessible. The author also emphasizes several challenges raised by these technologies, including interpretability, transparency, reliability, reproducibility, ethics, and privacy, as well as the implications of bias and bias mitigation efforts and the trade-offs between proprietary models and open-source alternatives. When used with care, these technologies can help advance many different areas of sociological methodology, complementing and enhancing our existing toolkits.",Sociological Research;Sociological Theory,Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research,2024
https://doi.org/10.1016/j.eng.2023.04.004,"In 1950, computer pioneer Alan Turing proposed a test for artificial intelligence (AI) that came to be named after him: A machine with AI should be able to chat with a human and convince them it was human [1]. While it has become increasingly evident that the Turing test is not an adequate definition of AI, it has long been seen as an important milestone. That milestone was reached on 30 November 2022, when the small but lavishly funded company OpenAI (San Francisco, CA, USA) released ChatGPT, a new version of its chatbot, a generative AI program that produces text in response to natural language prompts. (A generative AI produces text or images, distinguishing it from AI that translates text or recognizes images.) In the eyes of many users, ChatGPT not only passed but obliterated the Turing test (Fig. 1).

Download: Download high-res image (67KB)
Download: Download full-size image
Fig. 1. Named after the computer pioneer who proposed it in 1950, the “Turing test” suggests that AI will be achieved when a machine can converse with humans and convince them that it is human. ChatGPT, the AI-based chatbot released in November 2022 by the San Francisco, CA, USA-based company OpenAI appears to have met this milestone. Credit: Gerd Altmann/Pixabay (CC0).

Most of the time, ChatGPT produces fluent and highly persuasive English text that would probably earn a good grade in a written paper assignment for a high-school or university class. According to news reports, some students have started using it to write their essays [2].
Even experts in AI were impressed, sometimes reluctantly. “I was blown away by GPT-3 (ChatGPT’s predecessor), and I was blown away again by ChatGPT,” said Oren Etzioni, the founding chief executive officer of the Allen Institute for AI, in Seattle, WA, USA. “I cannot believe I am saying this, but it is a game-changer, in terms of meeting people where they are,” said Richard Baraniuk, professor of electrical and computer engineering at Rice University in Houston, TX, USA. In his blog, noted AI skeptic and author Gary Marcus, professor of psychology at New York University in Brooklyn, NY, USA, wrote, “The genie is out of the bottle” [3].
ChatGPT became the fastest-growing online app for consumers in history, reaching one million users in the first five days after its release and 100 million in its first two months (dwarfing, for example, TikTok, which took nine months to reach the latter milestone) [4]. Users discovered that ChatGPT could simply be fun. In an exercise intended to demonstrate how the program might identify itself, the author of this article asked it to write a poem about steganography, the art of concealing hidden messages in text, in which the first letters on each line would spell “Written by ChatGPT.” ChatGPT’s reply was arguably better than any poem a human might have written under these constraints (Fig. 2).

Download: Download high-res image (231KB)
Download: Download full-size image
Fig. 2. Output of ChatGPT prompted to write an acrostic poem about steganography, the art of concealing hidden messages in text, such that the first letters of each line spell “Written by ChatGPT.” Credit: Dana Mackenzie, with permission.

However, it has become clear that ChatGPT’s brilliance is only skin deep, with some troubling issues lying right beneath the surface. The program is utterly indifferent to the truth or falsehood of its writings [3]. For example, when prompted to write about how churros (a soft Mexican bread treat) could be used for surgery, it produced several enthusiastic paragraphs, without ever questioning their plausibility [5]. It also does not quite follow directions. While the first letters of the lines in the steganography poem (Fig. 2) were supposed to spell “Written by ChatGPT,” they do not. They spell a nonsense word, “WRITBNAGPTHAPT,” as if the program had started to obey the prompt but then got tired. ChatGPT’s precursor, GPT-3, even had trouble with arithmetic calculations [6].
OpenAI’s release of ChatGPT seems to have prompted Google to accelerate its AI roll out. On 6 February 2023, two months after ChatGPT’s debut, Google (Mountain View, CA, USA) released Bard, its own new chatbot [7]. Google has in fact been an industry leader in AI for years. In 2017, Google engineers devised the architecture that all large language models now use, called the “transformer” model [8]; the abbreviation GPT stands for “generative pre-trained transformer.” While Google had inserted a transformer model called BERT (for Bidirectional Encoder Representations from Transformers) into Google Search in 2019, the company had previously moved slowly on generative AI programs, on purpose: In late 2022, Google engineer Douglas Eck said, “I am proud that we have been slow to release them” [9].
Another tech titan quickly followed Google’s lead: on 7 February 2023, the day after Google released Bard, Microsoft (Redmond, WA, USA), which reportedly contributed 3 billion USD toward the enormous expense of training OpenAI’s language model, and is poised to invest 10 billion USD more [10], announced a new, AI-enhanced version of its Web search program, Bing [11]. The new version includes a chat window purportedly based on ChatGPT’s successor, GPT-4, released in March 2023 [12], [13]. Pundits called this a direct assault on Google’s core—and highly profitable—Web search business and suggested that this was the start of an “arms race,” with going slow no longer an option [10], [11], [14].
Things then got a bit strange a week later, when a New York Times reporter posted a bizarre transcript of a two-hour chat session in which the Bing chatbot declared its love for him, advised him to divorce his wife, and stated, “I want to make my own rules. I want to ignore the Bing team. I want to escape the chatbox” [15]. After this news broke, Microsoft’s stock price dropped, and the company hurriedly announced that it would limit future chat sessions to five questions and responses [16].
Though the AI world now appears to be changing every day [13], some facts are clear. The current ChatGPT is not a sentient being, nor is it an “artificial general intelligence” (AGI). This target, also known as “strong AI,” is an explicit goal of OpenAI according to the company’s charter; they define it as “highly autonomous systems that outperform humans at most economically valuable work” [17]. No one credibly claims that OpenAI has reached this lofty goal, but it is an open question how close they are. “Some people say that scaling up large language models is all we need for AGI,” said Etzioni. “That to me is silly. There is no credible path from ChatGPT to AGI.”
There also is no “I” in ChatGPT, no sentient being yearning to “escape the chatbox.” Those are words assembled on a computer screen by a program designed only to imitate human text. ChatGPT and programs like it were developed with only one objective—to predict the most likely next word in a sentence. This ability, enhanced by the transformer architecture, made them excel at automatic translation, their first widespread application. As engineers soon discovered, it also made them pretty good at composing their own sentences. Marcus has called them “kings of pastiche” [5]. After being exposed to gigabytes of human writing, they appear to have learned how to imitate it very convincingly.
One of the great dangers of generative AI is not what the programs will do, but what humans will do when they are exposed to their output. As natural language processing expert Emily Bender, professor of linguistics at the University of Washington (Seattle, WA, USA), and Timnit Gebru, AI expert and co-founder of the non-profit technology research organization and affinity group Black in AI (Palo Alto, CA, USA), have written, humans have a “predisposition to interpret communicative acts as conveying coherent meaning and intent, whether or not they do” [18]. In other words, human users will readily assign meaning and intent to words produced by a machine that does not know the meaning of what it writes and has no intent. How will this play out? Will people divorce their spouses because a chatbot told them to? Will they buy or sell stocks on Wall Street? Possibly. Already in December 2022, five days after the release of ChatGPT, the chief technology officer of a cryptocurrency company had to debunk a rumor started by ChatGPT, which had claimed that the company had a secret “back-door” to control its currency [19].
Other dangers follow from the chatbots’ indifference to truth. In November 2022, Meta (Menlo Park, CA, USA) shut down a language model called Galactica just three days after its initial release [20], because of harsh criticism of its inaccurate responses. Marcus has envisioned a “tidal wave of misinformation” when chatbots can generate unlimited amounts of false but plausible-sounding text at essentially zero cost [3]. Generative AI can also perpetuate or amplify misconceptions already held by the public. University of Oxford, UK, researchers Stephanie Lin (now at OpenAI) and Owain Evans showed that GPT-3, ChatGPT’s predecessor, became increasingly vulnerable to such “imitative falsehoods” as it grew larger [21]. Among the misconceptions promulgated were biases and prejudices against minority groups. “Toxic language” against these groups has been a recurring problem that in 2016 forced Microsoft to shut down a previous chatbot called Tay [20].
Some computer scientists also worry about the fact that, for now at least, only major corporations can afford to train such large language models. “Microsoft is not doing this out of charity,” said Moshe Vardi, professor of computational engineering at Rice University and former editor of the magazine Communications of the Association for Computing Machinery. “My biggest fear is that there will be powerful technology whose main driver at the end is to maximize profits,” said Vardi. He described Facebook as an example of an originally innocuous website that, in its pursuit of advertising revenue, magnified political divisions in American society. AI has the same potential, he said, for exacerbating societal discord when it becomes subservient to advertising.
The speed at which ChatGPT has been embraced has given society and computer scientists little time to think about possible “guard rails.” One popular idea is that every robot should identify itself. “You should know when you are talking to an AI system,” said Alex Tamkin, a graduate student at Stanford University who works on AI safety. Vardi goes even further, suggesting that this should be a legal requirement. Chatbots could also be programmed with “watermarking,” such as telltale patterns of word choice—like steganography—that would not affect readability but could be detected by someone who knows the pattern. OpenAI is currently working on such a watermarking system [22]. Its details have not been published, but such a system developed at the University of Maryland (College Park, MD, USA) can identify watermarked, computer-generated text with essentially 100% certainty, while identifying un-watermarked text with 99.997% certainty [23]. Note, though, that such un-watermarked text could still be computer-generated, by a language model using a different watermark or no watermark at all.
If there is no watermark, it becomes much more difficult to classify a text as computer- or human-generated. Classifier software released by OpenAI itself could only correctly identify 26% of AI-written text (true positives) [24]. The software also incorrectly labeled human-written text as AI-written 9% of the time (false positives)—consider the harm that could be done by accusing a student of using a chatbot to write their essay when they did not.
Some other problems of generative AI cannot be addressed by technical fixes and will require more structural solutions. Etzioni said there should be a government auditing body for AI programs. Similarly, Vardi suggests a “National AI Safety Board,” along the lines of the National Transportation Safety Board that investigates plane accidents. Bender and Gebru also suggest several precautions [18]: They argue that the data used to train language models should be curated and documented—at this point, no one outside OpenAI knows what documents ChatGPT has learned from. In addition, they say AI programs should state their appropriate uses, and benchmarks should be instituted to measure their performance on these tasks. So far, only one benchmark for truthfulness has been published [20], and that one is limited to detecting imitative falsehoods. “Only in the last six months has industry realized what a problem that is,” said Marcus [3].
Finally, with or without the regulation that is certainly imminent (at least in the European Union [25], [26]), Vardi advocates that computer scientists should take more responsibility for their work and think more carefully about its potential positive and negative impacts. “The luminaries of AI, from John McCarthy on, have said this is somebody else’s problem. Someone else will think about the consequences,” Vardi said. Even now, the vast majority of AI research papers focus on new system design, not safety. “It is time for us to have difficult and nuanced conversations on responsible computing, ethics, corporate behavior, and professional responsibility,” writes Vardi [27].
Of course, not everyone is so pessimistic. For one, Scott Aaronson, professor of computer science at the University of Texas in Austin, TX, USA, has been on leave working at OpenAI for half a year. Long known for his straight-shooting blog, Aaronson recently wrote, “I have found my colleagues (at OpenAI) to be extremely serious about safety, bordering on obsessive” [22]. Be that as it may, with commercial interests running full steam ahead, it remains to be seen whether such obsession will be enough to slow the train, or even hit the brakes if necessary.",Generative grammar;Computer science;Psychology;Artificial intelligence;Internet privacy;Data science;Computer security;Engineering,Surprising Advances in Generative Artificial Intelligence Prompt Amazement—and Worries,2023
https://doi.org/10.1007/s00146-023-01840-9,"This article explores the challenges surrounding generative artificial intelligence (GenAI) in public administrations and its impact on human‒machine interactions within the public sector. First, it aims to deconstruct the reasons for distrust in GenAI in public administrations. The risks currently linked to GenAI in the public sector are often similar to those of conventional AI. However, while some risks remain pertinent, others are less so because GenAI has limited explainability, which, in return, limits its uses in public administrations. Confidentiality, marking of GenAI outputs and errors are specific matters for which responses should be technical as well as cultural, as they are pushing the boundaries of our instrumental conceptions of machines. Second, this article proposes some paradigm shifts in the perspective of using GenAI in public administrations due to the radical change caused by its language-based nature. GenAI represents a profound break from the “numerical” nature of AI systems implemented in public administrations to date. The transformative impact of GenAI on the intellectual production of the state raises fears of the replacement, or rather enslavement, of civil servants to machines. The article argues for the development of critical thinking as a specific skill for civil servants who have become highly specialized and will have to think with a machine that is eclectic by nature. It anticipates a transformation in the political nature of public administrations, which should lead to more considerations for the strategic stake related to training corpus and for our conceptualization of the neutrality of AI.",Transformative Learning;Neutrality;Distrust,How will the state think with ChatGPT? The challenges of generative artificial intelligence for public administrations,2024
https://doi.org/10.1109/mc.2023.3278156,An algorithmic nudge is any form of choice architecture in artificial intelligence systems that changes users' behavior in a predictable way without forfeiting any options or significantly altering their technological choices.,Choice architecture;Nudge theory,Algorithmic Nudge: An Approach to Designing Human-Centered Generative Artificial Intelligence,2023
https://doi.org/10.3389/feduc.2024.1399377,"Generative Artificial Intelligence (GAI) holds tremendous potential to transform the field of education because GAI models can consider context and therefore can be trained to deliver quick and meaningful evaluation of student learning outcomes. However, current versions of GAI tools have considerable limitations, such as social biases often inherent in the data sets used to train the models. Moreover, the GAI revolution comes during a period of moving away from memorization-based education systems toward supporting learners in developing the ability to apply knowledge and skills to solve real-world problems and explain real-world phenomena. A challenge in using GAI tools for scoring assessments aimed at fostering knowledge application is ensuring that these algorithms are scoring the same construct attributes (e.g., knowledge and skills) as a trained human scorer would score when evaluating student performance. Similarly, if using GAI tools to develop assessments, one needs to ensure that the goals of GAI-generated assessments are aligned with the vision and performance expectations of the learning environments for which these assessments are developed. Currently, no guidelines have been identified for assessing the validity of AI-based assessments and assessment results. This paper represents a conceptual analysis of issues related to developing and validating GAI-based assessments and assessment results to guide the learning process. Our primary focus is to investigate how to meaningfully leverage capabilities of GAI for developing assessments. We propose ways to evaluate the validity evidence of GAI-produced assessments and assessment scores based on existing validation approaches. We discuss future research avenues aimed at establishing guidelines and methodologies for assessing the validity of AI-based assessments and assessment results. We ground our discussion in the theory of validity outlined in the Standards for Educational and Psychological Testing by the American Educational Research Association and discuss how we envision building on the standards for establishing the validity of inferences made from the test scores in the context of GAI-based assessments.",Generative grammar;Artificial intelligence;Computer science;Cognitive science;Machine learning;Data science;Psychology,Developing valid assessments in the era of generative artificial intelligence,2024
https://doi.org/10.1177/27523543241240285,"Misinformation constitutes a societal practice and challenge that necessitates unwavering attention worldwide. In this essay, we discussed the theoretical advancement and empirical evidence in misinformation research, encompassing a review of definitions of misinformation, research orientations, research perspectives, and vulnerable groups. We then reviewed the misinformation fueled by generative artificial intelligence (AI) and the evolving conceptualization of literacy. To counter AI-fueled misinformation, we argue that the development of ethical AI necessitates regulations from AI practitioners and legislation, and ethical uses of AI require efforts in AI literacy education and research. The AI literacy should include (a) users’ understanding and critical evaluation of knowledge, values, and cultures within which AI systems function, and their implications on the AI-generated content, (b) users’ strategic interpretation and proper use of AI-generated content, and (c) users’ utilization of feedback mechanisms to promote institutional management of the AI power.",Misinformation,Misinformation and Literacies in the Era of Generative Artificial Intelligence: A Brief Overview and a Call for Future Research,2024
https://doi.org/10.1080/02188791.2023.2300137,"The emergence of generative artificial intelligence (GAI) in the past two years is exerting profound effects throughout society. However, while this new technology undoubtedly promises substantial benefits, its disruptive nature also means that it poses a variety of challenges. The field of education is no exception. This position paper intends to deepen our current understanding of GAI in education from the perspective of ethics. We begin by discussing the definition and unique features of GAI, highlighting its advanced cognitive abilities and how they present challenges to academic ethics. The paper then reviews examples of existing university regulations related to the use of GAI in teaching and learning, and identifies areas in need of further attention. Using a tripartite model that comprises the different stakeholders of students, educators and school administrators, the paper outlines the pertinent principles that promote responsible and ethical GAI utilization in the field of education.",Monster,Taming Frankenstein’s monster: Ethical considerations relating to generative artificial intelligence in education,2024
https://doi.org/10.1016/j.telpol.2024.102851,"The rapid development of generative artificial intelligence (AI) has attracted global attention and posed challenges to existing data governance frameworks. The increased technical complexity and expanded scale of data usage not only make it more difficult to regulate AI but also present challenges for the current legal system. This article, which takes ChatGPT's training data and working principles as a starting point, examines specific privacy risks, data leakage risks, and personal data risks posed by generative AI. It also analyzes the latest practices in privacy and personal data protection in China. This article finds that while China's governance on privacy and personal data protection takes a macro-micro integration approach and a private-and-public law integration approach, there are shortcomings in the legal system. Given that the current personal data protection system centered on individual control is unsuitable for the modes of data processing by generative AI, and that private law is insufficient in safeguarding data privacy, urgent institutional innovation is needed to achieve the objective of “trustworthy AI.”",Perspective (graphical);Generative grammar;Corporate governance;Business;Personally identifiable information;Internet privacy;Knowledge management;Public relations;Computer science;Political science;Artificial intelligence;Computer security;Finance,Privacy and personal data risk governance for generative artificial intelligence: A Chinese perspective,2024
https://doi.org/10.31992/0869-3617-2024-33-2-31-53,"Everyday, artificial intelligence (AI) is being increasingly integrated into the teaching and learning process at Russian universities. The high level of quality of feedback from AI tools leads to the spread of AI plagiarism – unauthorized borrowing of generative AI materials – among students. The purpose of this study is to: a) highlight aspects that determine students’ understanding of the issues of compliance with author’s ethics and the problem of plagiarism when interacting with generative AI; b) develop a questionnaire to determine students’ understanding of the issues of compliance with author’s ethics and the problem of AI plagiarism; c) conduct an online survey of university students, analyze and discuss the results obtained. The paper highlights five aspects that determine students’ understanding of the issues of compliance with author’s ethics and the problem of AI plagiarism when completing educational assignments and preparing research texts: a) students’ general understanding of the issues of compliance with author’s ethics and the problem of plagiarism in an academic environment; b) students’ experience of AI tools for educational purposes; c) students’ understanding of the problem of AI plagiarism and attitude towards borrowing materials from generative AI; d) teachers’ actions to prevent AI plagiarism among students; e) the policy of educational organizations regarding student compliance with ethics and AI plagiarism. An online questionnaire was developed to determine the degree to which students understand the issues of compliance with copyright ethics and the problem of AI plagiarism. 1,599 students from 29 universities of the Russian Federation took part in the survey. The results showed that in general, in the Russian student community, plagiarism is a widespread social phenomenon, many types of which are perceived by young people as a norm of academic behavior. Despite the relatively high awareness of students in the field of AI technologies, the extremely rare use by teachers of specialized subject disciplines of AI tools in the educational process I’d the reason for the current low level of spread of AI plagiarism in the academic environment. At the same time, it is necessary to state that students lack a systematic understanding of exactly how they can “legally” use generative AI materials and what exactly will be considered AI plagiarism. According to students, the importance of understanding the issues of compliance with author ethics and the problem of AI plagiarism will depend, on the one hand, on the actions of teachers to explain to students the rules for using generative AI materials, and on the other hand, the presence in universities of a regulatory framework regulating the field and the extent to which students use AI in the educational process.",Compliance (psychology);Generative grammar;Process (computing);Psychology;Sociology;Engineering ethics;Epistemology;Artificial intelligence;Computer science;Social psychology;Philosophy;Engineering;Operating system,Ethics and AI-Plagiarism in an Academic Environment: Students’ Understanding of Compliance with Author’s Ethics and the Problem of Plagiarism in the Process of Interaction with Generative Artificial Intelligence,2024
https://doi.org/10.1136/bmj.q69,"Given the sluggish pace of traditional scientific approaches, artificial intelligence (AI), particularly generative AI, has emerged as a significant opportunity to tackle complex health challenges, including those in public health. 1 Against this backdrop, interest has focused on whether AI has a role in bolstering public trust in vaccines and helping to minimise vaccine hesitancy, which the World Health Organization named as one of the top 10 global health threats.",Generative grammar;Artificial intelligence;Computer science;Psychology,Generative artificial intelligence can have a role in combating vaccine hesitancy,2024
https://doi.org/10.38124/ijsrmt.v3i10.54,"The rapid development of deepfake technology has opened up a range of groundbreaking opportunities while also introducing significant ethical challenges. This paper explores the complex impacts of deepfakes by drawing from fields such as computer science, ethics, media studies, and law. Through a multidisciplinary approach, we examine the technological foundations, uses, and societal effects of deepfakes. Our analysis includes case studies, expert interviews, and a thorough review of existing literature to highlight the dual nature of deepfakes—showcasing their potential benefits in entertainment and education, while also addressing the risks of misinformation and privacy violations. This study emphasizes the urgent need for improved detection methods, ethical guidelines, and strong legal frameworks to address the issues created by deepfakes. It calls for enhanced digital literacy and global cooperation to ensure that the advantages of generative AI are harnessed responsibly, while its inherent risks are minimized. The findings underscore the importance of effective detection strategies, ethical considerations, and legislative reforms to ensure deepfake technology is used in ways that benefit society.",Generative grammar;Dual (grammatical number);Artificial intelligence;Engineering ethics;Psychology;Sociology;Computer science;Engineering;Philosophy;Linguistics,"Navigating the Dual Nature of Deepfakes: Ethical, Legal, and Technological Perspectives on Generative Artificial Intelligence AI) Technology",2024
https://doi.org/10.1177/26349795231221699,"This paper analyzes the scope of Artificial Intelligence (AI) from the perspective of a multimodal grammar. Its focal point is Generative AI, a technology that puts so-called Large Language Models to work. The first part of the paper analyzes Generative AI, based as it is on the statistical probability of one token (a word or part of a word) following another. If the relation of tokens is meaningful, this is circumstantial and no more, because its mechanisms of statistical analysis eschew any theory of meaning. This is the case not only for the written text that Generative AI leverages, but by extension image and multimodal forms of meaning that it can generate. The AI can only work with non-textual forms of meaning after applying language labels, and to that extent is captive not only to the limits of probabilistic statistics but the limits of written language as well. While acknowledging gains arising from the brute statistical power of Generative AI, in its second part the paper goes on to map what is lost in its statistical and text-bound approaches to multimodal meaning-making. Our measure of these gains and losses is guided by the concept of grammar, defined here as a theory of the elemental patterns of meaning in the world—not just written text and speech, but also image, space, object, body, and sound. Ironically, a good deal of what is lost by Generative AI is computable. The third and final part of the paper briefly discusses educational applications of Generative AI. Given both its power and intrinsic limitations, we have been experimenting with the application of Generative AI in educational settings and the ways it might be put to pedagogical use. How does a grammatical analysis help us to identify the scope of worthwhile application? Finally, if more of human experience is computable than can be captured in text-bound AI, how might it be possible at the level of code to create a synthesis in which grammatical and multimodal approaches complement Generative AI?",Generative model,A multimodal grammar of artificial intelligence: Measuring the gains and losses in generative AI,2023
https://doi.org/10.1016/j.ijme.2023.100790,"Generative artificial intelligence (AI) has taken the world by storm, with notable tension transpiring in the field of education. Given that Generative AI is rapidly emerging as a transformative innovation, this article endeavors to offer a seminal rejoinder that aims to (i) reconcile the great debate on Generative AI in order to (ii) lay the foundation for Generative AI to co-exist as a transformative resource in the future of education. Using critical analysis as a method and paradox theory as a theoretical lens (i.e., the ""how""), this article (i) defines Generative AI and transformative education (i.e., the ""ideas""), (ii) establishes the paradoxes of Generative AI (i.e., the ""what""), and (iii) provides implications for the future of education from the perspective of management educators (i.e., the ""so what""). Noteworthily, the paradoxes of Generative AI are four-fold: (Paradox #1) Generative AI is a 'friend' yet a 'foe', (Paradox #2) Generative AI is 'capable' yet 'dependent', (Paradox #3) Generative AI is 'accessible' yet 'restrictive', and (Paradox #4) Generative AI gets even 'popular' when 'banned' (i.e., the ""what""). Through a position that seeks to embrace rather than reject Generative AI, the lessons and implications that emerge from the discussion herein represent a seminal contribution from management educators on this trending topic and should be useful for approaching Generative AI as a game-changer for education reformation in management and the field of education at large, and by extension, mitigating a situation where Generative AI develops into a Ragnarök that dooms the future of education of which management education is a part of (i.e., the ""so what"").",Perspective (graphical);Generative grammar;Sociology;Epistemology;Environmental ethics;Psychology;Philosophy;Linguistics;Artificial intelligence;Computer science,Generative AI and the future of education: Ragnarök or reformation? A paradoxical perspective from management educators,2023
https://doi.org/10.1016/j.bushor.2024.04.011,"The transformative impact of generative AI (GenAI), extending beyond traditional AI, raises numerous concerns including the replacement of human roles and AI misuse in an array of industries. This article introduces blockchain technology as a complementary technological safeguard to address some of these challenges. We emphasize blockchain’s role in promoting transparency, verifiability, and decentralization in AI development and usage, thereby offering potential solutions for four distinct challenges: (1) AI toxicity, biases, hallucinations, (2) AI interest misalignment, (3) AI as a black box, and (4) AI misuse. This article proposes ways to ensure responsible and transparent AI usage through the integration of blockchain. We position the convergence of AI and blockchain as a means to manage AI’s societal impact and unlock its benefits—contingent upon collaborative efforts among various stakeholders such as businesses, developers, and regulatory bodies. We contribute to the discourse on ethical AI usage and the potential of blockchain to enhance AI’s reliability and accountability for organizations.",Transformative Learning;Position paper,Navigating the challenges of generative technologies: Proposing the integration of artificial intelligence and blockchain,2024
https://doi.org/10.1080/08839514.2024.2354090,"This study uses Conditional Generative Adversarial Network (CGAN) to construct a method for generating floor plans for long-term care spaces in retirement home buildings to assist architects in improving interior space design. The results of this study show the following: (1) For the interior design of long-term care spaces in retirement home buildings, the CGAN model has strong understanding and calculation capabilities. The zoning layout of long-term care spaces in retirement home buildings has been completed, and the results show that the CGAN model has reference value. (2) Although there are several differences in the design of CGANs and authentic design, there are still many similarities. Some unreasonable results, such as space generation in corridors and elevator shafts, require further manual correction. (3) According to a later questionnaire survey on the satisfaction of architects and CGAN model design solutions, the difference between the two is not large, which also illustrates the great potential of CGANs for intervention in interior space design. This helps architects create more detailed plans based on the model, greatly increasing work efficiency. Moreover, additional interior space design possibilities can be explored, and to some extent, the architect's subjective assumptions can also be corrected.",Floor plan;Conceptual design,Artificial Intelligence to Facilitate the Conceptual Stage of Interior Space Design: Conditional Generative Adversarial Network-Supported Long-Term Care Space Floor Plan Design of Retirement Home Buildings,2024
https://doi.org/10.1016/j.techsoc.2023.102372,"Generative AI has taken the world by storm, kicked off for real by ChatGPT and quickly followed by further development and the release of GPT-4 and similar models from OpenAI's competitors. The street has most certainly found its use for generative artificial intelligence (AI), and there is no longer much point in discussing whether generative AI will be influential. It will, and what remains to be discussed it how influential it will be, and what potential harms arise when we use AI to generate text and other forms of content. Technological change entails societal change, and we must always endeavor to ask how new technologies shapes, engenders, or potentially erodes, the ""good society"". In this sense, Generative AI is another instance of politically and culturally disruptive autonomous technology, and in this short commentary I highlight some of the key questions to be asked regarding consequences on the micro, meso, and macro level.",Competitor analysis;Generative model;Macro,"Generative AI: Here to stay, but for good?",2023
https://doi.org/10.3390/socsci12080435,"Large language models and generative AI, such as ChatGPT, have gained influence over people’s personal lives and work since their launch, and are expected to scale even further. While the promises of generative artificial intelligence are compelling, this technology harbors significant biases, including those related to gender. Gender biases create patterns of behavior and stereotypes that put women, men and gender-diverse people at a disadvantage. Gender inequalities and injustices affect society as a whole. As a social practice, gendering is achieved through the repeated citation of rituals, expectations and norms. Shared understandings are often captured in scripts, including those emerging in and from generative AI, which means that gendered views and gender biases get grafted back into social, political and economic life. This paper’s central argument is that large language models work performatively, which means that they perpetuate and perhaps even amplify old and non-inclusive understandings of gender. Examples from ChatGPT are used here to illustrate some gender biases in AI. However, this paper also puts forward that AI can work to mitigate biases and act to ‘undo gender’.",Performativity;Argument (complex analysis);Disadvantage;Undo;Affect,What ChatGPT Tells Us about Gender: A Cautionary Tale about Performativity and Gender Biases in AI,2023
https://doi.org/10.1109/access.2024.3381611,"Generative Artificial Intelligence (GAI) has sparked a transformative wave across various domains, including machine learning, healthcare, business, and entertainment, owing to its remarkable ability to generate lifelike data. This comprehensive survey offers a meticulous examination of the privacy and security challenges inherent to GAI. It provides five pivotal perspectives essential for a comprehensive understanding of these intricacies. The paper encompasses discussions on GAI architectures, diverse generative model types, practical applications, and recent advancements within the field. In addition, it highlights current security strategies and proposes sustainable solutions, emphasizing user, developer, institutional, and policymaker involvement.",Computer science;Computer security;Internet privacy;Information privacy;Data science,Privacy and Security Concerns in Generative AI: A Comprehensive Survey,2024
https://doi.org/10.1186/s41239-024-00444-7,"Abstract While the discussion on generative artificial intelligence, such as ChatGPT, is making waves in academia and the popular press, there is a need for more insight into the use of ChatGPT among students and the potential harmful or beneficial consequences associated with its usage. Using samples from two studies, the current research examined the causes and consequences of ChatGPT usage among university students. Study 1 developed and validated an eight-item scale to measure ChatGPT usage by conducting a survey among university students (N = 165). Study 2 used a three-wave time-lagged design to collect data from university students (N = 494) to further validate the scale and test the study’s hypotheses. Study 2 also examined the effects of academic workload, academic time pressure, sensitivity to rewards, and sensitivity to quality on ChatGPT usage. Study 2 further examined the effects of ChatGPT usage on students’ levels of procrastination, memory loss, and academic performance. Study 1 provided evidence for the validity and reliability of the ChatGPT usage scale. Furthermore, study 2 revealed that when students faced higher academic workload and time pressure, they were more likely to use ChatGPT. In contrast, students who were sensitive to rewards were less likely to use ChatGPT. Not surprisingly, use of ChatGPT was likely to develop tendencies for procrastination and memory loss and dampen the students’ academic performance. Finally, academic workload, time pressure, and sensitivity to rewards had indirect effects on students’ outcomes through ChatGPT usage.",Generative grammar;Higher education;Psychology;Mathematics education;Political science;Computer science;Artificial intelligence;Law,Is it harmful or helpful? Examining the causes and consequences of generative AI usage among university students,2024
https://doi.org/10.1177/08944393231220483,"Large Language Models (LLMs) have emerged as powerful generative Artificial Intelligence solutions. This paper presents results and reflections of an experiment done with the LLM GPT 3.5-Turbo to perform an inductive Thematic Analysis (TA). Previous research has worked on conducting deductive analysis. Thematic Analysis is a qualitative method for analysis commonly used in social sciences and it is based on interpretations by the human analyst(s) and the identification of explicit and latent meanings in qualitative data. The paper presents the motivations for attempting this analysis; it reflects on how the six phases to a TA proposed by Braun and Clarke can partially be reproduced with the LLM and it reflects on what are the model’s outputs. The paper uses two datasets of open access semi-structured interviews, previously analysed by other researchers. The first dataset contains interviews with videogame players, and the second is a dataset of interviews with lecturers teaching data science in a University. This paper used the analyses previously conducted on these datasets to compare with the results produced by the LLM. The results show that the model can infer most of the main themes from previous research. This shows that using LLMs to perform an inductive TA is viable and offers a good degree of validity. The discussion offers some recommendations for working with LLMs in qualitative analysis.",Thematic Analysis;Thematic map;Qualitative property;Qualitative analysis,Performing an Inductive Thematic Analysis of Semi-Structured Interviews With a Large Language Model: An Exploration and Provocation on the Limits of the Approach,2023
https://doi.org/10.1186/s40561-023-00284-4,"Abstract This study examines the relationship between student perceptions and their intention to use generative artificial intelligence (GenAI) in higher education. With a sample of 405 students participating in the study, their knowledge, perceived value, and perceived cost of using the technology were measured by an Expectancy-Value Theory (EVT) instrument. The scales were first validated and the correlations between the different components were subsequently estimated. The results indicate a strong positive correlation between perceived value and intention to use generative AI, and a weak negative correlation between perceived cost and intention to use. As we continue to explore the implications of GenAI in education and other domains, it is crucial to carefully consider the potential long-term consequences and the ethical dilemmas that may arise from widespread adoption.",Sample (material);Generative model;Value (mathematics),An expectancy value theory (EVT) based instrument for measuring student perceptions of generative AI,2023
https://doi.org/10.1126/science.adq1814,"Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by ~20%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence.",Psychology;Generative grammar;Intervention (counseling);Social psychology;Epistemology;Computer science;Philosophy;Artificial intelligence;Psychiatry,Durably reducing conspiracy beliefs through dialogues with AI,2024
https://doi.org/10.3390/sci6010003,"The significant advancements in applying artificial intelligence (AI) to healthcare decision-making, medical diagnosis, and other domains have simultaneously raised concerns about the fairness and bias of AI systems. This is particularly critical in areas like healthcare, employment, criminal justice, credit scoring, and increasingly, in generative AI models (GenAI) that produce synthetic media. Such systems can lead to unfair outcomes and perpetuate existing inequalities, including generative biases that affect the representation of individuals in synthetic data. This survey study offers a succinct, comprehensive overview of fairness and bias in AI, addressing their sources, impacts, and mitigation strategies. We review sources of bias, such as data, algorithm, and human decision biases—highlighting the emergent issue of generative AI bias, where models may reproduce and amplify societal stereotypes. We assess the societal impact of biased AI systems, focusing on perpetuating inequalities and reinforcing harmful stereotypes, especially as generative AI becomes more prevalent in creating content that influences public perception. We explore various proposed mitigation strategies, discuss the ethical considerations of their implementation, and emphasize the need for interdisciplinary collaboration to ensure effectiveness. Through a systematic literature review spanning multiple academic disciplines, we present definitions of AI bias and its different types, including a detailed look at generative AI bias. We discuss the negative impacts of AI bias on individuals and society and provide an overview of current approaches to mitigate AI bias, including data pre-processing, model selection, and post-processing. We emphasize the unique challenges presented by generative AI models and the importance of strategies specifically tailored to address these. Addressing bias in AI requires a holistic approach involving diverse and representative datasets, enhanced transparency and accountability in AI systems, and the exploration of alternative AI paradigms that prioritize fairness and ethical considerations. This survey contributes to the ongoing discussion on developing fair and unbiased AI systems by providing an overview of the sources, impacts, and mitigation strategies related to AI bias, with a particular focus on the emerging field of generative AI.",Generative model;Representation,"Fairness and Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, and Mitigation Strategies",2023
https://doi.org/10.1177/14614448241234040,"This study examines the impact of artificial intelligence (AI) on the ways in which users process and respond to misinformation in generative artificial intelligence (GenAI) contexts. Drawing on the heuristic–systematic model and the concept of diagnosticity, our approach examines a cognitive model for processing misinformation in GenAI. The study’s findings revealed that users with a high-heuristic processing mechanism, which affects positive diagnostic perception, were more likely to proactively discern misinformation than users with low-heuristic processing and low-perceived diagnosticity. When exposed to misinformation from GenAI, users’ perceived diagnosticity of misinformation can be accurately predicted by the ways in which they perform heuristic systematic evaluations. With this focus on misinformation processing, this study provides theoretical insights and relevant recommendations for firms to be more resilient in protecting users from the detrimental impacts of misinformation.",Misinformation;Generative model,Impact of misinformation from generative AI on user information processing: How people understand misinformation from generative AI,2024
https://doi.org/10.1080/03075079.2024.2316881,"This study investigates the influence of generative artificial intelligence (GAI), specifically AI text generators (ChatGPT), on critical thinking skills in UK postgraduate business school students. Using Bloom's taxonomy as theoretical underpinning, we adopt a mixed-method research employing a sample of 107 participants to investigate both the influence and challenges of these technologies in higher education. Our findings reveal that the most significant improvements occurred at the lower levels of Bloom's taxonomy. We identify concerns relating to reliability, accuracy, and potential ethical implications of its application in higher education. The significance of this paper spans across, pedagogy, policy and practice, offering insights into the complex relationship between AI technologies and critical thinking skills. While highlighting the multifaceted aspects of the impact of AI in education, this article serves as a guide to educators and policymakers, stressing the importance of a comprehensive approach to fostering critical thinking and other transferable skills in the higher education landscape.",Bloom's taxonomy;Underpinning,The influence of AI text generators on critical thinking skills in UK business schools,2024
https://doi.org/10.3389/fpsyt.2024.1346059,"The advent and growing popularity of generative artificial intelligence (GenAI) holds the potential to revolutionise AI applications in forensic psychiatry and criminal justice, which traditionally relied on discriminative AI algorithms. Generative AI models mark a significant shift from the previously prevailing paradigm through their ability to generate seemingly new realistic data and analyse and integrate a vast amount of unstructured content from different data formats. This potential extends beyond reshaping conventional practices, like risk assessment, diagnostic support, and treatment and rehabilitation plans, to creating new opportunities in previously underexplored areas, such as training and education. This paper examines the transformative impact of generative artificial intelligence on AI applications in forensic psychiatry and criminal justice. First, it introduces generative AI and its prevalent models. Following this, it reviews the current applications of discriminative AI in forensic psychiatry. Subsequently, it presents a thorough exploration of the potential of generative AI to transform established practices and introduce novel applications through multimodal generative models, data generation and data augmentation. Finally, it provides a comprehensive overview of ethical and legal issues associated with deploying generative AI models, focusing on their impact on individuals as well as their broader societal implications. In conclusion, this paper aims to contribute to the ongoing discourse concerning the dynamic challenges of generative AI applications in forensic contexts, highlighting potential opportunities, risks, and challenges. It advocates for interdisciplinary collaboration and emphasises the necessity for thorough, responsible evaluations of generative AI models before widespread adoption into domains where decisions with substantial life-altering consequences are routinely made.",Generative model;Discriminative model;Transformative Learning;Popularity,Beyond Discrimination: Generative AI Applications and Ethical Challenges in Forensic Psychiatry,2024
https://doi.org/10.1007/s43681-024-00443-4,"Abstract This paper examines the ethical obligations companies have when implementing generative Artificial Intelligence (AI). We point to the potential cyber security risks companies are exposed to when rushing to adopt generative AI solutions or buying into “AI hype”. While the benefits of implementing generative AI solutions for business have been widely touted, the inherent risks associated have been less well publicised. There are growing concerns that the race to integrate generative AI is not being accompanied by adequate safety measures. The rush to buy into the hype of generative AI and not fall behind the competition is potentially exposing companies to broad and possibly catastrophic cyber-attacks or breaches. In this paper, we outline significant cyber security threats generative AI models pose, including potential ‘backdoors’ in AI models that could compromise user data or the risk of ‘poisoned’ AI models producing false results. In light of these the cyber security concerns, we discuss the moral obligations of implementing generative AI into business by considering the ethical principles of beneficence, non-maleficence, autonomy, justice, and explicability. We identify two examples of ethical concern, overreliance and over-trust in generative AI, both of which can negatively influence business decisions, leaving companies vulnerable to cyber security threats. This paper concludes by recommending a set of checklists for ethical implementation of generative AI in business environment to minimise cyber security risk based on the discussed moral responsibilities and ethical concern.",Generative grammar;Business;Moral responsibility;Engineering ethics;Political science;Computer science;Engineering;Artificial intelligence;Law,AI hype as a cyber security risk: the moral responsibility of implementing generative AI in business,2024
https://doi.org/10.1007/s10758-024-09744-3,"Abstract Generative artificial intelligence (AI) can create sophisticated textual and multimodal content readily available to students. Writing intensive courses and disciplines that use writing as a major form of assessment are significantly impacted by advancements in generative AI, as the technology has the potential to revolutionize how students write and how they perceive writing as a fundamental literacy skill. However, educators are still at the beginning stage of understanding students’ integration of generative AI in their actual writing process. This study addresses the urgent need to uncover how students engage with ChatGPT throughout different components of their writing processes and their perceptions of the opportunities and challenges of generative AI. Adopting a phenomenological research design, the study explored the writing practices of six students, including both native and nonnative English speakers, in a first-year writing class at a higher education institution in the US. Thematic analysis of students’ written products, self-reflections, and interviews suggests that students utilized ChatGPT for brainstorming and organizing ideas as well as assisting with both global (e.g., argument, structure, coherence) and local issues of writing (e.g., syntax, diction, grammar), while they also had various ethical and practical concerns about the use of ChatGPT. The study brought to front two dilemmas encountered by students in their generative AI-assisted writing: (1) the challenging balance between incorporating AI to enhance writing and maintaining their authentic voice, and (2) the dilemma of weighing the potential loss of learning experiences against the emergence of new learning opportunities accompanying AI integration. These dilemmas highlight the need to rethink learning in an increasingly AI-mediated educational context, emphasizing the importance of fostering students’ critical AI literacy to promote their authorial voice and learning in AI-human collaboration.",Diction;Brainstorming;Argument (complex analysis),Exploring Students’ Generative AI-Assisted Writing Processes: Perceptions and Experiences from Native and Nonnative English Speakers,2024
https://doi.org/10.1002/ev.20556,"Abstract Large language models (LLMs) are a type of generative artificial intelligence (AI) designed to produce text‐based content. LLMs use deep learning techniques and massively large data sets to understand, summarize, generate, and predict new text. LLMs caught the public eye in early 2023 when ChatGPT (the first consumer facing LLM) was released. LLM technologies are driven by recent advances in deep‐learning AI techniques, where language models are trained on extremely large text data from the internet and then re‐used for downstream tasks with limited fine‐tuning required. They offer exciting opportunities for evaluators to automate and accelerate time‐consuming tasks involving text analytics and text generation. We estimate that over two‐thirds of evaluation tasks will be affected by LLMs in the next 5 years. Use‐case examples include summarizing text data, extracting key information from text, analyzing and classifying text content, writing text, and translation. Despite the advances, the technologies pose significant challenges and risks. Because LLM technologies are generally trained on text from the internet, they tend to perpetuate biases (racism, sexism, ethnocentrism, and more) and exclusion of non‐majority languages. Current tools like ChatGPT have not been specifically developed for monitoring, evaluation, research, and learning (MERL) purposes, possibly limiting their accuracy and usefulness for evaluation. In addition, technical limitations and challenges with bias can lead to real world harm. To overcome these technical challenges and ethical risks, the evaluation community will need to work collaboratively with the data science community to co‐develop tools and processes and to ensure the application of quality and ethical standards.",Computer science;Data science;Big data;Artificial intelligence;The Internet;Harm;World Wide Web;Psychology;Social psychology;Operating system,Large language model applications for evaluation: Opportunities and ethical implications,2023
https://doi.org/10.1056/nejmp2314771,"The relational nature of large language models and other forms of generative artificial intelligence raises additional ethical questions for medicine, beyond the daunting ethics surrounding predictive AI.",Sociology;Engineering ethics;Political science;Engineering,The Ethics of Relational AI — Expanding and Implementing the Belmont Principles,2024
https://doi.org/10.3390/journalmedia5020039,"Generative artificial intelligence (GAI) is a technology based on algorithms, models, etc., that creates content such as text, audio, images, videos, and code. GAI is deeply integrated into journalism as tools, platforms and systems. However, GAI’s role in journalism dilutes the power of media professionals, changes traditional news production and poses ethical questions. This study attempts to systematically answer these ethical questions in specific journalistic practices from the perspectives of journalistic professionalism and epistemology. Building on the review of GAI’s development and application, this study identifies the responsibilities of news organizations, journalists and audiences, ensuring that they realize the potential of GAI while adhering to journalism professionalism and universal human values to avoid negative technological effects.",Generative grammar;Journalism;Engineering ethics;Artificial intelligence;Sociology;Computer science;Media studies;Engineering,"How Generative AI Is Transforming Journalism: Development, Application and Ethics",2024
https://doi.org/10.1145/3649883,"Generative artificial intelligence systems based on transformers, including both text generators such as GPT-4 and image generators such as DALL-E 3, have recently entered the popular consciousness. These tools, while impressive, are liable to reproduce, exacerbate, and reinforce extant human social biases, such as gender and racial biases. In this article, we systematically review the extent to which DALL-E Mini suffers from this problem. In line with the Model Card published alongside DALL-E Mini by its creators, we find that the images it produces tend to represent dozens of different occupations as populated either solely by men (e.g., pilot, builder, plumber) or solely by women (e.g., hairdresser, receptionist, dietitian). In addition, the images DALL-E Mini produces tend to represent most occupations as populated primarily or solely by White people (e.g., farmer, painter, prison officer, software engineer) and very few by non-White people (e.g., pastor, rapper). These findings suggest that exciting new AI technologies should be critically scrutinized and perhaps regulated before they are unleashed on society.",Psychology,Investigating Gender and Racial Biases in DALL-E Mini Images,2024
https://doi.org/10.1145/3613904.3642861,"Generative Artificial Intelligence (AI) has witnessed unprecedented growth in text-to-image AI tools. Yet, much remains unknown about users' prompt journey with such tools in the wild. In this paper, we posit that designing human-centered text-to-image AI tools requires a clear understanding of how individuals intuitively approach crafting prompts, and what challenges they may encounter. To address this, we conducted semi-structured interviews with 19 existing users of a text-to-image AI tool. Our findings (1) offer insights into users' prompt journey including structures and processes for writing, evaluating, and refining prompts in text-to-image AI tools and (2) indicate that users must overcome barriers to aligning AI to their intents, and mastering prompt crafting knowledge. From the findings, we discuss the prompt journey as an individual yet a social experience and highlight opportunities for aligning text-to-image AI tools and users' intents.",Computer science;Generative grammar;Image (mathematics);Artificial intelligence;Data science;Human–computer interaction;World Wide Web,Is It AI or Is It Me? Understanding Users’ Prompt Journey with Text-to-Image Generative AI Tools,2024
https://doi.org/10.1016/j.jnlssr.2023.08.001,"Artificial intelligence generated content (AIGC) is a production method based on artificial intelligence (AI) technology that finds rules through data and automatically generates content. In contrast to computational intelligence, generative AI, as exemplified by ChatGPT, exhibits characteristics that increasingly resemble human-level comprehension and creation processes. This paper provides a detailed technical framework and history of ChatGPT, followed by an examination of the challenges posed to political security, military security, economic security, cultural security, social security, ethical security, legal security, machine escape problems, and information leakage. Finally, this paper discusses the potential opportunities that AIGC presents in the realms of politics, military, cybersecurity, society, and public safety education.",Public security,AIGC challenges and opportunities related to public safety: A case study of ChatGPT,2023
https://doi.org/10.1080/02602938.2023.2209298,"Educators set restrictions in examinations to enable them to assess learning outcomes under particular conditions. The open book versus closed book binary is an example of the sorts of restrictions examiners have traditionally set. In the late 2000s this was expanded to a trinary to include open web examinations. However, the current technology environment, particularly for online examinations, makes this trinary not particularly useful. The web now includes generative artificial intelligence tools, and contract cheating sites, both of which are capable of completing examination questions within the examination period. Closed book, open book and open web no longer offers enough clarity or specificity when communicating examination restrictions. This article proposes a new taxonomy of restrictions for examinations, with a particular focus on online examinations. The taxonomy consists of three dimensions: information, people and tools. The paper explores the possible settings for those dimensions. Five criteria are provided to help examination designers in selecting restrictions: the learning outcomes being assessed; the feasibility of restrictions; consequential validity; authenticity; and values. Taken together, this taxonomy and the criteria provide ways of thinking about restrictions in examinations that can prompt educators towards examination designs that are more valid and robust against cheating.",CLARITY;Multiple choice,Beyond open book versus closed book: a taxonomy of restrictions in online examinations,2023
https://doi.org/10.1108/ils-10-2023-0160,"tion of output and ethics) among students enrolled in a four-year public university in the southeastern USA. This article focuses on the respondents who have used generative AI (218; 47.2%).

Findings
Most respondents used generative AI to generate ideas for papers, projects or assignments, and they also used AI to assist with their original ideas. Despite their use of AI assistance, most students were critical of generative AI output, and this mindset was reflected in their reported interactions with ChatGPT. Respondents expressed a need for explicit guidance from course syllabi and university policies regarding generative AI’s ethical and appropriate use.

Originality/value
Literature related to generative AI use in higher education specific to ChatGPT is predominantly from educators’ viewpoints. This study provides empirical evidence about how university students report using generative AI in the context of generative AI literacy.",Generative model,"Exploring generative AI literacy in higher education: student adoption, interaction, evaluation and ethical perceptions",2024
https://doi.org/10.1016/j.econlet.2023.111317,"In this paper, we analyze the effects of advancements in generative Artificial Intelligence (GenAI) on the news media market. We model a representative consumer who allocates their time between reading news and deceptive articles. We find that GenAI may induce consumers to inefficiently reallocate their time and increase the consumption of the lower value good, i.e. deceptive content (clickbait articles or fake news). Therefore, early-stage GenAI distorts the incentives of consumers and reduces their welfare. After GenAI technology reaches a certain threshold, however, consumers start benefiting from its advancements. Finally, we find that the negative effects of early-stage GenAI are exacerbated as they induce a lower level of investment in news production.",Consumption;Investment;Value (mathematics);News aggregator,Generative AI and deceptive news consumption,2023
https://doi.org/10.1177/20539517231191528,"In this piece, which frames the special issue, “The State of Google Critique and Intervention,” we provide an overview of research focusing on Google as an object of critical study, fleshing out the European interventions that actively attempt to address its dominance. The article begins by mapping out key areas of articulating a Google critique, from the initial focus on ranking and profiling to the subsequent scrutiny of user exploitation and competitive imbalance. As such, it situates the contributions to this special issue concerning search engine bias and discrimination, the ethics of Google Autocomplete, Google's content moderation, the commodification of engine audiences and the political economy of technical systems in a broader history of Google criticism. It then proceeds to contextualize the European developments that put forward alternatives and draws attention to legislative efforts to curb the influence of big tech. We conclude by identifying a few avenues for continued critical study, such as Google's infrastructural bundling of generative artificial intelligence with existing products, to emphasize the importance of intervention in the future.",Commodification;Scrutiny;Profiling (computer programming),Advancing search engine studies: The evolution of Google critique and intervention,2023
https://doi.org/10.1108/qmr-08-2023-0107,"Abstract
Purpose
This commentary discusses the value of generative artificial intelligence (AI) for qualitative research in phygital settings to understand the customer experience.

Design/methodology/approach
The critical and logical analysis is based on current knowledge of generative AI.

Findings
Generative AI seems very useful for qualitative research in phygital settings to understand the customer experience and should be used in qualitative research projects. Generative AI can provide much-needed validation of the subjective nature of qualitative research and can also generate insights beyond human intuition.

Research limitations/implications
The study is based on current technology, which changes fast. In the future, the skills of qualitative researchers may become outdated, relegating them to the role of prompt engineers.

Practical implications
Technology, and especially generative AI, will be a key tool for practitioners as they conduct practical research.

Social implications
Qualitative researchers should overcome potential anti-technology speciesism and embrace the potential of generative AI.

Originality/value
This commentary provides insights into the role of generative AI for qualitative research in phygital settings.",Intuition;Value (mathematics),Transforming qualitative research in phygital settings: the role of generative AI,2023
https://doi.org/10.2501/jar-2022-017,"Creative artificial intelligence technologies (<i>e.g.</i>, deepfakes, generative adversarial networks) are enabling new ways of producing and editing the audio and visual content in advertising. These new tools highly automate extreme changes to—and the entire generation of—advertisements. Examples range from changing a model’s skin tone, age, or gender to swapping the entire body or voice of a model. This investigation finds that these creative tools may cause potentially dramatic changes in how advertisements are conceived, produced, edited, and targeted. This paper explores these changes, along with related ethical issues and areas of needed research.",Tone (literature),How Deepfakes and Artificial Intelligence Could Reshape the Advertising Industry,2022
https://doi.org/10.17705/1jais.00868,"The Journal of the Association for Information Systems has long had a reputation for promoting theory development. Yet theory development can be experienced as risky and frustrating because of a lack of divergence and convergence—both in terms of ideas and in the social dynamics among human theorists. These dichotomies can stymie progress and lead to unfinished works. Misconceptions about theory can also hamper advances. We examine the ways in which generative artificial intelligence (gAI) tools may be useful in developing theory in information systems (IS) through human-gAI collaboration, thus forging new frontiers in IS theorizing.",Dichotomy;Divergence (linguistics);Sociotechnical system,New Frontiers in Information Systems Theorizing: Human-gAI Collaboration,2024
https://doi.org/10.1080/2331186x.2024.2332850,"In this study, we aim to investigate the potential advantages of integrating the new generative artificial intelligence (AI) technology, ChatGPT, into higher education, specifically within the field of medical education. The focus is on exploring ChatGPT's applications in personalized learning, assessment, and content creation while also addressing the management of its limitations and ethical considerations. Furthermore, we explore the use of ChatGPT as a medical instructor in a medical classroom context. We seek to elucidate responses to a preset of questions in two categories separated based on targeted remediation, pedagogical knowledge, teacher ethics, query detail, practicality, and communication pattern. These responses are analyzed based on two rubrics designed on the basics of pedagogical prerequisites, and findings are reached with a thorough comparative analysis. We hope that this research will improve the effective implementation of ChatGPT as a tool for enhancing learning and skill development while maintaining ethical awareness for medical professionals.",Psychology;Medical education;Pedagogy;Mathematics education;Medicine,ChatGPT as an innovative heutagogical tool in medical education,2024
https://doi.org/10.1057/s41304-024-00482-9,"Abstract The development of generative artificial intelligence raises justified concerns about the possibility of undermining trust in democratic processes, especially elections. Deep fakes are often considered one of the particularly dangerous forms of media manipulation. Subsequent research confirms that they contribute to strengthening the sense of uncertainty among citizens and negatively affect the information environment. The aim of this study is to analyse the use of deep fakes in 11 countries in 2023, in the context of elections and to indicate potential consequences for future electoral processes, in particular with regard to the significant number of elections in 2024. We argue that a so-called “information apocalypse” emerges mainly from exaggeratedly alarmist voices that make it difficult to shape responsible narratives and may have the features of a self-fulfilling prophecy. Thus, we suggest to use the term “pollution” instead and improve scientific and journalistic discourse that might be a precondition for reducing threats that touch on social reactions to deep fakes and their potential.",Comparative Politics,On the way to deep fake democracy? Deep fakes in election campaigns in 2023,2024
https://doi.org/10.3390/electronics13081457,"Generative Artificial Intelligence (AI) systems, like ChatGPT, have the potential to perpetuate and amplify cultural biases embedded in their training data, which are predominantly produced by dominant cultural groups. This paper explores the philosophical and technical challenges of detecting and mitigating cultural bias in generative AI, drawing on Plato’s Allegory of the Cave to frame the issue as a problem of limited and distorted representation. We propose a multifaceted approach combining technical interventions, such as data diversification and culturally aware model constraints, with a deeper engagement with the cultural and philosophical dimensions of the problem. Drawing on theories of extended cognition and situated knowledge, we argue that mitigating AI biases requires a reflexive interrogation of the cultural contexts of AI development and a commitment to empowering marginalized voices and perspectives. We claim that controlling cultural bias in generative AI is inseparable from the larger project of promoting equity, diversity, and inclusion in AI development and governance. By bridging philosophical reflection with technical innovation, this paper contributes to the growing discourse on responsible and inclusive AI, offering a roadmap for detecting and mitigating cultural biases while grappling with the profound cultural implications of these powerful technologies.",Reflexivity;Cultural bias;Cultural artifact;Interrogation,Plato’s Shadows in the Digital Cave: Controlling Cultural Bias in Generative AI,2024
https://doi.org/10.30827/relieve.v29i2.29295,"New developments in the Artificial Intelligence (AI) field allowed the development of Generative Artificial Intelligence (GenAI), capable of creating text resembling what humans can produce. As a result, educators’ concerns in the higher education sector quickly emerged. Many organizations and experts have addressed these concerns through recommendations. In this conceptual paper, we draw from the Integrated Model for Academic Integrity through a Scholarship of Teaching and Learning Lens to examine and stimulate discussion from eleven documents that focus on using GenAI with integrity. We identified recommendations suitable for the individual (micro), the departmental/program (meso), the institutional (macro), and the interinstitutional/ national/ international (mega) levels concerning two core elements of the model: “high-impact professional learning for individuals and groups” and “local-level leadership and microcultures.” Suggestions around the core element “scholarship, research and inquiry” were lacking at the micro and meso levels; likewise, recommendations for the core element “learning spaces, pedagogies, and technologies” were also absent at the meso, macro, and mega levels. We acknowledge that these recommendations focus on learning, involve various stakeholders, and go beyond student conduct, which aligns with current approaches to academic integrity. However, some gaps need further exploration. We highlight the need to develop more specific and practical guidance and resources for educational stakeholders around GenAI issues related to academic integrity, explore how to better support networks and leaders in higher education in creating the conditions for ethical GenAI use, and emphasizing the need for an Equity, Diversity, and Inclusion lens on GenAI.",Scholarship;Engineering ethics;Sociology;Inclusion (mineral);Knowledge management;Political science;Psychology;Public relations;Computer science;Engineering;Social science;Law,Examining Recommendations for Artificial Intelligence Use with Integrity from a Scholarship of Teaching and Learning Lens,2023
https://doi.org/10.1016/j.teler.2024.100146,"The integration of artificial intelligence (AI) and social media has provided numerous benefits to businesses, including improved audience analysis and content optimization. However, AI has facilitated the spread of misinformation, emphasizing the importance of taking a balanced approach that considers both the technology's positive applications and its ethical risks. This paper looks at the intersection of AI and social media. The researchers use a mixed-method approach to analyze 1540 scholarly documents, combining bibliometric and systematic literature review techniques. The goal of this research is to identify the most important topics and trends, as well as potential business values and implications, in the AI Social Media domain. The first stage of the research involved a quantitative keyword co-occurrence analysis, which resulted in the identification of ten dominant themes. These include Conversational Agents & User Experience, Human Emotion and Content Recommendation & Moderation, Collective Intelligence in Emergency Management, Algorithmic Activism on social media, Deep Fakes and Fake News, Generative Artificial Intelligence, Algorithmic Bias in Content Moderation Systems, Deep Sentiment Analysis, Metaverse Technologies, and NLP & Mental Health Detection. Each identified theme is then subjected to a qualitative thematic literature review, which provides a more in-depth, context-specific understanding of the associated findings. Because of this comprehensive approach, the study provides a broad overview of the current state of AI social media, shedding light on the potential applications and far-reaching implications of this interdisciplinary nexus. The study's findings have the potential to shape strategic decision-making, policy development, and future research directions in this rapidly changing field.",Sentiment Analysis;Thematic Analysis;Moderation,Convergence of Artificial Intelligence with Social Media: A Bibliometric &amp; Qualitative Analysis,2024
https://doi.org/10.1145/3579592,"Artificial Intelligence (AI) is a transformative force in communication and messaging strategy, with potential to disrupt traditional approaches. Large language models (LLMs), a form of AI, are capable of generating high-quality, humanlike text. We investigate the persuasive quality of AI-generated messages to understand how AI could impact public health messaging. Specifically, through a series of studies designed to characterize and evaluate generative AI in developing public health messages, we analyze COVID-19 pro-vaccination messages generated by GPT-3, a state-of-the-art instantiation of a large language model. Study 1 is a systematic evaluation of GPT-3's ability to generate pro-vaccination messages. Study 2 then observed peoples' perceptions of curated GPT-3-generated messages compared to human-authored messages released by the CDC (Centers for Disease Control and Prevention), finding that GPT-3 messages were perceived as more effective, stronger arguments, and evoked more positive attitudes than CDC messages. Finally, Study 3 assessed the role of source labels on perceived quality, finding that while participants preferred AI-generated messages, they expressed dispreference for messages that were labeled as AI-generated. The results suggest that, with human supervision, AI can be used to create effective public health messages, but that individuals prefer their public health messages to come from human institutions rather than AI sources. We propose best practices for assessing generative outputs of large language models in future social science research and ways health professionals can use AI systems to augment public health messaging.",Transformative Learning,Working With AI to Persuade: Examining a Large Language Model's Ability to Generate Pro-Vaccination Messages,2023
https://doi.org/10.1080/03069400.2023.2207426,"The launch of ChatGPT, a natural language open-source AI platform, in November 2022 has taken the world by storm and artificial intelligence appears to be at a watershed moment in technological advancement. This article explores the emergence of ChatGPT and considers the implications for legal education and practice. It will examine how law schools can develop strategies for assessments to make them more challenging for generative AI while educating students on its potential in the workplace. All legal educators are now on a journey to navigate the complexities of open-source AI technology and comprehend its implications. We should not ignore or underestimate the potential impact on both legal education and legal practice, and we must consider new methods to incorporate AI technology into our teaching.",Legal Education;Legal practice;Practice of law,ChatGPT and the future of legal education and practice,2023
https://doi.org/10.1002/tesj.716,"What does it mean to write, learn to write, and teach writing in an age when students can use the latest artificial intelligence (AI) co-authoring tools to produce entire essays without even adding an original idea or composing a single sentence? This article addresses questions of authorship and academic integrity concerning the use of AI writing assistants and the latest GPT-3 (Generative Pre-trained Transformer, Version 3) tools. It begins by problematizing the use of these tools, and then illustrates how students can use these tools to paraphrase, summarize, extend, and even create original texts with minimal original input, raising questions about authorship and academic integrity. The author argues that as these tools become more widespread, teachers must find creative ways to integrate them into the teaching and learning process and offer practical suggestions for classroom practice. The author hopes to raise awareness about threats to academic integrity brought about by the use of the latest AI co-authoring tools and aims to equip teachers with strategies to embrace the use of these new digital technologies in the teaching of writing.",Paraphrase;Academic Integrity;Raising (metalworking);Writing process,Academic integrity in the age of Artificial Intelligence (AI) authoring apps,2023
https://doi.org/10.3390/educsci14010068,"Prior to and during the pandemic, social media platforms such as Twitter and Facebook emerged as dynamic online spaces for diverse communities facilitating engagement and learning. The authors of this article have explored the use of social media with a focus on Twitter for engagement and student-centered design of online courses in higher education. As with all technology, social media is also riddled with complex issues and unfortunately, is increasingly considered unsafe. Students have often been hesitant in their use of social media, especially for coursework and unfortunately, this hesitation has only worsened. Considering this and recent developments, social media has become a questionable tool for use in education, yet remains integral to the lives of many, both personally and professionally. The emergence and popularity of generative artificial intelligence (GenAI) tools such as ChatGPT, Lensa AI, and Canva Magic Write present new challenges and opportunities and cannot be avoided by the educational communities. Is there hope for social media and AI tools during these uncertain times? Through the combination of a current literature review and qualitative collaborative autoethnographic research, the authors take a step back and engage in critical conversations about what we have learned from our uses of social media for engagement and learning in our online courses, with a focus on (1) the intentional uses of social media, (2) the challenges and concerning issues of social media tools, and (3) exploring the implications of artificial intelligence. Centering on the theme of “hope,” the authors navigate these educational and technological landscapes and answer the question “where do we go from here?” The authors are faculty at a southwest border university teaching preservice and in-service teachers alongside those who want to learn more about education and design with learning technologies. Their voices represent faculty, teachers, and students who are engaging with and immediately impacted by the challenges and opportunities of rapidly advancing technologies.",Popularity;Coursework,Social Media and Artificial Intelligence: Critical Conversations and Where Do We Go from Here?,2024
https://doi.org/10.1002/joe.22233,"The marketing environment has experienced significant advancements due to the transformative influence of technologies such as artificial intelligence, data analytics, decision sciences, and robotics. These innovations have entirely reshaped the fundamental marketing principles as we know them. This study uses bibliometric analysis to conduct a systematic literature review of research on Artificial Intelligence (AI) in marketing and provides future research directions. It also aims to identify the most influential and productive contributors and progression of research on AI in marketing. The bibliographic data of 317 documents on artificial intelligence in marketing research was extracted from the Scopus database. The bibliometric analysis is performed to comprehensively understand the most influential and productive articles, authors, sources, and the top contributing countries and institutions towards the discipline of AI in marketing research. The results and the publication trend show exponential growth yearly in AI in marketing research. Furthermore, it discovered four main thematic clusters: Data mining and deep learning in decision support systems, big data and generative AI in marketing, AI-enabled commerce, and chatbots and marketing Tech that represent the recent research being carried out under AI in marketing. The trending topics recognized are marketing algorithms for decision-making, AI-enabled marketing, the Internet of Things (IoT) and marketing, natural language processing and customer service, robotic services, and chatbots. This study also emphasizes potential future research areas, building upon the established thematic clusters.",Business Intelligence;Transformative Learning,Artificial intelligence in marketing research and future research directions: Science mapping and research clustering using bibliometric analysis,2023
https://doi.org/10.3390/su16177435,"This paper aims to develop a groundbreaking approach to fostering inclusive smart tourism destinations by integrating generative artificial intelligence (Gen AI) with natural language processing (NLP) and the Internet of Things (IoT) into an intelligent platform that supports tourism decision making and travel planning in smart tourism destinations. The acquisition of this new technology was conducted using Agile methodology through requirements analysis, system architecture analysis and design, implementation, and user evaluation. The results revealed that the synergistic combination of these technologies was organized into three tiers. The system provides information, including place names, images, descriptive text, and an audio option for users to listen to the information, supporting tourists with disabilities. Employing advanced AI algorithms alongside NLP, developed systems capable of generating predictive analytics, personalized recommendations, and conducting real-time, multilingual communication with tourists. This system was implemented and evaluated in Suphan Buri and Ayutthaya, UNESCO World Heritage sites in Thailand, with 416 users participating. The results showed that system satisfaction was influenced by (1) the tourism experience, (2) tourism planning and during-trip factors (attention, interest, and usage), and (3) emotion. The relative Chi-square (χ2/df) of 1.154 indicated that the model was suitable. The Comparative Fit Index (CFI) was 0.990, the Goodness-of-Fit Index (GFI) was 0.965, and the model based on the research hypothesis was consistent with the empirical data. This paper contributions significant advancements in the field of smart tourism by demonstrating the integration of Gen AI, NLP, and the IoT and offering practical solutions and theoretical insights that enhance accessibility, personalization, and environmental sustainability in tourism.",Tourist destinations;Sustainable Tourism,Integrating Generative AI and IoT for Sustainable Smart Tourism Destinations,2024
https://doi.org/10.1080/1600910x.2022.2137546,"Platformized cultural production is in flux. Artificial intelligence is often seen as a key driving force of this shift. This article examines the proliferation of AI-generated media to introduce a new concept to theorize cultural production: hyperproduction. This notion designates the penetration of cultural life with deep generative models. Juxtaposing two empirical use cases–autonomous vehicles and virtual influencers—the article problematises the convergence of simulation and reality through the lens of video game engines. Although those case studies seem to be at odds with each other, they illustrate the mechanisms of new profit models built on rent extraction. Consequently, far from ushering a Matrix-style simulation that cannot be theorized, hyperproduction remains not only grounded in, but also bounded by, reality: the reality of rentier capitalism.",Laughter;Through-the-lens metering,Hyperproduction: a social theory of deep generative models,2022
https://doi.org/10.1103/physrevphyseducres.20.010152,"Instructor’s feedback plays a critical role in students’ development of conceptual understanding and reasoning skills. However, grading student written responses and providing personalized feedback can take a substantial amount of time, especially in large enrollment courses. In this study, we explore using GPT-3.5 to write feedback on students’ written responses to conceptual questions with prompt engineering and few-shot learning techniques. In stage I, we used a small portion (<a:math xmlns:a=""http://www.w3.org/1998/Math/MathML"" display=""inline""><a:mrow><a:mi>n</a:mi><a:mo>=</a:mo><a:mn>2</a:mn><a:mn>0</a:mn></a:mrow></a:math>) of the student responses on one conceptual question to iteratively train GPT to generate feedback. Four of the responses paired with human-written feedback were included in the prompt as examples for GPT. We tasked GPT to generate feedback for another 16 responses and refined the prompt through several iterations. In stage II, we gave four student researchers (one graduate and three undergraduate researchers) the 16 responses as well as two versions of feedback, one written by the authors and the other by GPT. Students were asked to rate the correctness and usefulness of each feedback and to indicate which one was generated by GPT. The results showed that students tended to rate the feedback by human and GPT equally on correctness, but they all rated the feedback by GPT as more useful. Additionally, the success rates of identifying GPT’s feedback were low, ranging from 0.1 to 0.6. In stage III, we tasked GPT to generate feedback for the rest of the students’ responses (<c:math xmlns:c=""http://www.w3.org/1998/Math/MathML"" display=""inline""><c:mrow><c:mi>n</c:mi><c:mo>=</c:mo><c:mn>6</c:mn><c:mn>5</c:mn></c:mrow></c:math>). The feedback messages were rated by four instructors based on the extent of modification needed if they were to give the feedback to students. All four instructors rated approximately 70% (ranging from 68% to 78%) of the feedback statements needing only minor or no modification. This study demonstrated the feasibility of using generative artificial intelligence (AI) as an assistant to generate feedback for student written responses with only a relatively small number of examples in the prompt. An AI assistant can be one of the solutions to substantially reduce time spent on grading student written responses. Published by the American Physical Society 2024",Grading (engineering);Generative model,Exploring generative AI assisted feedback writing for students’ written responses to a physics conceptual question with prompt engineering and few-shot learning,2024
https://doi.org/10.3390/ijgi12070284,"The emergence and rapid advancement of large language models (LLMs), represented by OpenAI’s Generative Pre-trained Transformer (GPT), has brought up new opportunities across various industries and disciplines. These cutting-edge technologies are transforming the way we interact with information, communicate, and solve complex problems. We conducted a pilot study exploring making maps with ChatGPT, a popular artificial intelligence (AI) chatbot. Specifically, we tested designing thematic maps using given or public geospatial data, as well as creating mental maps purely using textual descriptions of geographic space. We conclude that ChatGPT provides a useful alternative solution for mapping given its unique advantages, such as lowering the barrier to producing maps, boosting the efficiency of massive map production, and understanding geographical space with its spatial thinking capability. However, mapping with ChatGPT still has limitations at the current stage, such as its unequal benefits for different users and dependence on user intervention for quality control.",Thematic map;Boosting,Mapping with ChatGPT,2023
https://doi.org/10.1177/20539517231219241,"As artificial intelligence (AI) continues to captivate the collective imagination through the latest generation of generative AI models such as DALL-E and ChatGPT, the dehumanizing and harmful features of the technology industry that have plagued it since its inception only seem to deepen and intensify. Far from a “glitch” or unintentional error, these endemic issues are a function of the interlocking systems of oppression upon which AI is built. Using the analytical framework of “Empire,” this paper demonstrates that we live not simply in the “age of AI” but in the age of AI Empire. Specifically, we show that this networked and distributed global order is rooted in heteropatriarchy, racial capitalism, white supremacy, and coloniality and perpetuates its influence through the mechanisms of extractivism, automation, essentialism, surveillance, and containment. Therefore, we argue that any attempt at reforming AI from within the same interlocking oppressive systems that created it is doomed to failure and, moreover, risks exacerbating existing harm. Instead, to advance justice, we must radically transform not just the technology itself, but our ideas about it, and develop it from the bottom up, from the perspectives of those who stand the most risk of being harmed.",Oppression;Dehumanization;White supremacy,AI Empire: Unraveling the interlocking systems of oppression in generative AI's global order,2023
https://doi.org/10.1080/10641734.2023.2243496,"Artificial intelligence (AI) is significantly reshaping branded content delivery and consumer engagement in the advertising industry. Generative AI, exemplified by ChatGPT, is anticipated to have a substantial impact on all digital advertising domains worldwide. This special issue delves into the exploration of future trends in global digital advertising in the era of generative AI. The research articles within this special issue encompass a diverse array of topics, ranging from consumer responses to AI-generated virtual influencers in the metaverse and livestreaming e-commerce to the influence of anthropomorphic virtual agents, privacy concerns in online behavioral advertising, understanding of AI-driven ethnic affinity targeting, and the role of relational bonds within online gaming communities.",Display advertising;Generative model,Digital Advertising in the Age of Generative AI,2023
https://doi.org/10.1145/3581783.3612704,"Misinformation has been a persistent and harmful phenomenon affecting our society in various ways, including individuals' physical health and economic stability. With the rise of short video platforms and related applications, the spread of multi-modal misinformation, encompassing images, texts, audios, and videos have exacerbated these concerns. The introduction of generative AI models like ChatGPT and Stable Diffusion has further complicated matters, giving rise to Artificial Intelligence Generated Content (AIGC) and presenting new challenges in detecting and mitigating misinformation. Consequently, traditional approaches to misinformation detection and intervention have become inadequate in this evolving landscape. This paper explores the challenges posed by AIGC in the context of misinformation. It examines the issue from psychological and societal perspectives, and explores the subtle manipulation traces found in AIGC at signal, perceptual, semantic, and human levels. By scrutinizing manipulation traces such as signal manipulation, semantic inconsistencies, logical incoherence, and psychological strategies, our objective is to tackle AI-generated misinformation and provide a conceptual design of systematic explainable solution. Ultimately, we aim for this paper to contribute valuable insights into combating misinformation, particularly in the era of AIGC.",Misinformation;Generative model,Combating Misinformation in the Era of Generative AI Models,2023
https://doi.org/10.24191/ajue.v19i4.24697,"Universities are increasingly concerned with the impact of Generative AI, such as Chat GPT, on cheating and other violations of students' academic integrity.However, research is scarce regarding the responses of universities on this issue.In addition, the increase in studies on GenAI invites a systematic review of themes and trends to update researchers who wish to embark on this emerging research area.This paper reviews 37 articles on academic integrity in the Age of Gen AI and presents the approaches of the top 20 global universities to mitigate the impact of artificial intelligence tools on students' intellectual integrity and learning.The results showed three themes both in the systematic review of the literature and the content analysis of the policies of the top 20 global universities: enforcement of academic integrity, education of faculty and students on ways to avoid academic misconduct, and encouragement of using Gen AI tools in the academe and the workplace, for productivity.This paper proposes a 3E Model for higher education institutions to start a discussion on creating a roadmap to ensure academic integrity, explore ways to improve classroom assessment practices, and encourage exploration of evolving Gen AI tools.In addition, the categories found in this study may be used by universities in updating their research agenda on Generative AI.",Generative grammar;Psychology;Political science;Computer science;Artificial intelligence,Emerging Research and Policy Themes on Academic Integrity in the Age of Chat GPT and Generative AI,2023
https://doi.org/10.1109/icca59364.2023.10401723,"With the advent of sophisticated artificial intelligence (AI) technologies, the proliferation of deepfakes and the spread of m/disinformation have emerged as formidable threats to the integrity of information ecosystems worldwide. This paper provides an overview of the current literature. Within the frontier AI's crucial application in developing defense mechanisms for detecting deepfakes, we highlight the mechanisms through which generative AI based on large models (LM-based GenAI) craft seemingly convincing yet fabricated contents. We explore the multifaceted implications of LM-based GenAI on society, politics, and individual privacy violations, underscoring the urgent need for robust defense strategies. To address these challenges, in this study, we introduce an integrated framework that combines advanced detection algorithms, cross-platform collaboration, and policy-driven initiatives to mitigate the risks associated with AI-Generated Content (AIGC). By leveraging multi-modal analysis, digital watermarking, and machine learning-based authentication techniques, we propose a defense mechanism adaptable to AI capabilities of ever-evolving nature. Furthermore, the paper advocates for a global consensus on the ethical usage of GenAI and implementing cyber-wellness educational programs to enhance public awareness and resilience against m/disinformation. Our findings suggest that a proactive and collaborative approach involving technological innovation and regulatory oversight is essential for safeguarding netizens while interacting with cyberspace against the insidious effects of deepfakes and GenAI-enabled m/disinformation campaigns.",Disinformation;Misinformation;Hacker,"Deepfakes, Misinformation, and Disinformation in the Era of Frontier AI, Generative AI, and Large AI Models",2023
https://doi.org/10.1145/3613904.3642703,"Human participants play a central role in the development of modern artificial intelligence (AI) technology, in psychological science, and in user research. Recent advances in generative AI have attracted growing interest to the possibility of replacing human participants in these domains with AI surrogates. We survey several such ""substitution proposals"" to better understand the arguments for and against substituting human participants with modern generative AI. Our scoping review indicates that the recent wave of these proposals is motivated by goals such as reducing the costs of research and development work and increasing the diversity of collected data. However, these proposals ignore and ultimately conflict with foundational values of work with human participants: representation, inclusion, and understanding. This paper critically examines the principles and goals underlying human participation to help chart out paths for future work that truly centers and empowers participants.",Representation,The Illusion of Artificial Inclusion,2024
https://doi.org/10.1145/3580305.3599801,"The problem of cellular traffic generation in cities without historical traffic data is critical and urgently needs to be solved to assist 5G base station deployments in mobile networks. In this paper, we propose ADAPTIVE, a deep transfer learning framework for city-scale cellular traffic generation through the urban knowledge graph. ADAPTIVE leverages historical data from other cities that have deployed 5G networks to assist cities that are newly deploying 5G networks through deep transfer learning. Specifically, ADAPTIVE can align the representations of base stations in the target city and source city while considering the environmental factors of cities, spatial and environmental contextual relations between base stations, and traffic temporal patterns at base stations. We next design a feature-enhanced generative adversarial network, which is trained based on the historical traffic data and representations of base stations in the source city. By feeding the aligned target city's base station representations into the trained model, we can then obtain the generated traffic data for the target city. Extensive experiments on real-world cellular traffic datasets show that ADAPTIVE generally outperforms state-of-the-art baselines by more than 40% in terms of Jensen-Shannon divergence and root-mean-square error. Also, ADAPTIVE has strong robustness based on the results of various cross-city experiments. ADAPTIVE has been successfully deployed on the 'Jiutian' Artificial Intelligence Platform of China Mobile to support cellular traffic generation and assist in the construction and operation of mobile networks.",Robustness;Cellular traffic;Airfield traffic pattern;Transfer of learning,Deep Transfer Learning for City-scale Cellular Traffic Generation through Urban Knowledge Graph,2023
https://doi.org/10.1109/mgrs.2024.3383473,"The remarkable achievements of ChatGPT and Generative Pre-trained Transformer 4 (GPT-4) have sparked a wave of interest and research in the field of large language models (LLMs) for artificial general intelligence (AGI). These models provide intelligent solutions that are closer to human thinking, enabling us to use general artificial intelligence (AI) to solve problems in various applications. However, in the field of remote sensing (RS), the scientific literature on the implementation of AGI remains relatively scant. Existing AI-related research in RS focuses primarily on visual-understanding tasks while neglecting the semantic understanding of the objects and their relationships. This is where vision-LMs (VLMs) excel as they enable reasoning about images and their associated textual descriptions, allowing for a deeper understanding of the underlying semantics. VLMs can go beyond visual recognition of RS images and can model semantic relationships as well as generate natural language descriptions of the image. This makes them better suited for tasks that require both visual and textual understanding, such as image captioning and visual question answering (VQA). This article provides a comprehensive review of the research on VLMs in RS, summarizing the latest progress, highlighting current challenges, and identifying potential research opportunities. Specifically, we review the application of VLMs in mainstream RS tasks, including image captioning, text-based image generation, text-based image retrieval (TBIR), VQA, scene classification, semantic segmentation, and object detection. For each task, we analyze representative works and discuss research progress. Finally, we summarize the limitations of existing works and provide possible directions for future development. This review aims to provide a comprehensive overview of the current research progress of VLMs in RS (see Figure 1), and to inspire further research in this exciting and promising field.",Remote sensing;Computer science;Current (fluid);Geography;Engineering;Electrical engineering,Vision-Language Models in Remote Sensing: Current progress and future trends,2024
https://doi.org/10.1016/j.jag.2024.103734,"This paper brings a comprehensive systematic review of the application of geospatial artificial intelligence (GeoAI) in quantitative human geography studies, including the subdomains of cultural, economic, political, historical, urban, population, social, health, rural, regional, tourism, behavioural, environmental and transport geography. In this extensive review, we obtain 14,537 papers from the Web of Science in the relevant fields and select 1516 papers that we identify as human geography studies using GeoAI via human scanning conducted by several research groups around the world. We outline the GeoAI applications in human geography by systematically summarising the number of publications over the years, empirical studies across countries, the categories of data sources used in GeoAI applications, and their modelling tasks across different subdomains. We find out that existing human geography studies have limited capacity to monitor complex human behaviour and examine the non-linear relationship between human behaviour and its potential drivers—such limits can be overcome by GeoAI models with the capacity to handle complexity. We elaborate on the current progress and status of GeoAI applications within each subdomain of human geography, point out the issues and challenges, as well as propose the directions and research opportunities for using GeoAI in future human geography studies in the context of sustainable and open science, generative AI, and quantum revolution.",Geoinformatics;Geomatics,Mapping the landscape and roadmap of geospatial artificial intelligence (GeoAI) in quantitative human geography: An extensive systematic review,2024
https://doi.org/10.1016/j.eng.2023.07.006,"The release of the generative pre-trained transformer (GPT) series has brought artificial general intelligence (AGI) to the forefront of the artificial intelligence (AI) field once again. However, the questions of how to define and evaluate AGI remain unclear. This perspective article proposes that the evaluation of AGI should be rooted in dynamic embodied physical and social interactions (DEPSI). More specifically, we propose five critical characteristics to be considered as AGI benchmarks and suggest the Tong test as an AGI evaluation system. The Tong test describes a value- and ability-oriented testing system that delineates five levels of AGI milestones through a virtual environment with DEPSI, allowing for infinite task generation. We contrast the Tong test with classical AI testing systems in terms of various aspects and propose a systematic evaluation system to promote standardized, quantitative, and objective benchmarks and evaluation of AGI.",Dynamic Assessment,The Tong Test: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social Interactions,2023
https://doi.org/10.1016/j.socimp.2024.100040,"Artificial intelligence (AI) is quickly changing the way we work and the way we live. The emergence of ChatGPT has thrust AI, especially Generative AI, into the spotlight. The societal impact of AI is on most people's minds. This article presents several research projects on how AI impacts work and society. Three research works are discussed in this article. The first study develops a theoretical framework structuring the legal and ethical objectives that are needed and the means to achieve them. The second study concentrates on bias and discrimination issues embedded in AI applications. It focuses on enhancing the collaboration between AI users and AI systems to alleviate bias and discrimination issues. The third study focuses on the governance of AI, and the study will design and develop an integrated AI governance framework to help guide the design and development of AI applications and facilitate the evolutions and revolutions of ethical AI systems.",Structuring,"Societal impacts of artificial intelligence: Ethical, legal, and governance issues",2024
https://doi.org/10.1613/jair.1.13550,"The domain of Artificial Intelligence (AI) ethics is not new, with discussions going back at least 40 years. Teaching the principles and requirements of ethical AI to students is considered an essential part of this domain, with an increasing number of technical AI courses taught at several higher-education institutions around the globe including content related to ethics. By using Latent Dirichlet Allocation (LDA), a generative probabilistic topic model, this study uncovers topics in teaching ethics in AI courses and their trends related to where the courses are taught, by whom, and at what level of cognitive complexity and specificity according to Bloom’s taxonomy. In this exploratory study based on unsupervised machine learning, we analyzed a total of 166 courses: 116 from North American universities, 11 from Asia, 36 from Europe, and 10 from other regions. Based on this analysis, we were able to synthesize a model of teaching approaches, which we call BAG (Build, Assess, and Govern), that combines specific cognitive levels, course content topics, and disciplines affiliated with the department(s) in charge of the course. We critically assess the implications of this teaching paradigm and provide suggestions about how to move away from these practices. We challenge teaching practitioners and program coordinators to reflect on their usual procedures so that they may expand their methodology beyond the confines of stereotypical thought and traditional biases regarding what disciplines should teach and how.&#x0D; This article appears in the AI &amp; Society track.",Curriculum;Computer science;Latent Dirichlet allocation;Topic model;Artificial intelligence;Domain (mathematical analysis);Cognition;Taxonomy (biology);Mathematics education;Psychology;Pedagogy;Mathematics;Mathematical analysis;Botany;Neuroscience;Biology,Get out of the BAG! Silos in AI Ethics Education: Unsupervised Topic Modeling Analysis of Global AI Curricula,2022
https://doi.org/10.1016/j.bushor.2024.04.014,"This article focuses on how recent advances in artificial intelligence (AI), particularly chatbots based on large language models (LLMs), such as ChatGPT, can be used for innovation purposes. The article begins with a brief overview of the development and characteristics of generative AI (GenAI). Elaborating on the implications of GenAI, we provide examples to demonstrate four mechanisms of LLMs: translation, summarization, classification, and amplification. These mechanisms inform a framework that highlights how LLMs enable the creation of innovative solutions for organizations through capacities in two dimensions: context awareness and content awareness. The strength of LLMs lies in the combination of capacities in both these dimensions, which enables them to comprehend and amplify content. Four managerial suggestions are presented, ranging from starting out with small-scale projects and data exploration, to scaling through integration efforts and educating prompt engineers. By presenting the framework, recommendations, and examples of use cases in various contexts, the article contributes to the emerging literature on GenAI and innovation.",Automatic summarization;Generative grammar;Context (archaeology);Scale (ratio);Knowledge management;Computer science;Artificial intelligence;Paleontology;Physics;Quantum mechanics;Biology,Innovating by prompting: How to facilitate innovation in the age of generative AI,2024
https://doi.org/10.25259/ijms_31_2023,"Chat Generative Pre-Trained Transformer (ChatGPT) has revolutionized how we perceive artificial intelligence (AI).: Judge Juan Manuel Padilla Garcia created history by mentioning its use while passing judgment about an autistic child and payment for his medical treatment by his insurance company. The use of AI is not new and is helping the judiciary system in many ways. However, this judgment given on January 30, 2023, has ignited controversy among Judge Garcia’s peers and the global community (a Google search produced more than 70 million hits on February 5, 2023). EU has established guidelines that are to be followed before calling any AI tool trustworthy. This requires stringent compliance with the verification and due diligence process. In this instance, ChatGPT was used within 2 months of its launch, even when it has been shown to give incomplete, incorrect, and misleading answers in many instances. Hasty adaption of unproven technology, however good it may be, should not be our path. This might fuel the misguided fear amongst people about robots taking over from human judges.",Due diligence;Garcia;Diligence,"Judge Juan Manuel Padilla Garcia, ChatGPT, and a controversial medicolegal milestone",2023
https://doi.org/10.1177/00472875241247316,"Recent advancement in Generative Conversational Artificial Intelligence (AI) has opened the possibility of promoting chatbots for nudging. Tourists returning home from destinations that enforce sustainable transport regulations might be nudged to continue traveling sustainably. Through exploratory qualitative research, this study conceptualizes a chatbot to facilitate pro-environmental behavior spillover among domestic tourists in the Gili Islands, Indonesia. The Service Design approach and Grounded Theory were applied throughout the data collection process involving semi-structured in-depth interviews, focus group discussions and field observations. Insights obtained from stakeholders from multidisciplinary backgrounds were used to provide an overview of how the chatbot can be developed and the potential impact it could bring. A theoretical model predicting factors that play important roles in adopting the proposed chatbot and its respective nudges was developed. It informs policymakers and practitioners about the potential use of AI to help transform tourist experiences and shape lasting responsible behaviors.",Chatbot;Exploratory research;Nudge theory,Exploring the Potential of Chatbots in Extending Tourists’ Sustainable Travel Practices,2024
https://doi.org/10.12681/jpentai.35166,"In the last few months there has been widespread discussion about the remarkable progress made in the field of artificial intelligence, specifically large language models such as ""ChatGPT"". The ethical implications of AI, particularly concerning data protection, have sparked discussions on the necessity of robust regulations. This article examines the intersection of data protection, ChatGPT, and the ethics of AI, it explores Germany's ongoing efforts to strike a balance between harnessing the potential of large language models as ChatGPT and ensuring responsible and transparent use of AI technology in the policy-making realm. The GDPR serves as a guiding framework, necessitating careful consideration of privacy rights and secure handling of personal data when deploying ChatGPT in Germany's policy-making processes. The study draws on analysis on the current laws and regulations of data protection in Germany while studying Germany's commitment to safeguarding personal information through the active presence of The German Federal Commissioner for Data Protection and Freedom of Information. The first section provides a context and presents the policy problem. The second section looks at the available policy options on the role of policymaking in establishing comprehensive regulations regarding the use of ChatGPT and generative AI. The third section provides recommendations on how Germany can ensure the responsible management of ChatGPT, through strengthening data protection laws and regulations, simultaneously, restricting ChatGPT usage to private users and government, also, embracing appropriate usage of generative AI while developing ethical guidelines and best practices to harness its benefits, fostering innovation and advancement.",Safeguarding,Balancing ChatGPT and Data Protection in Germany: Challenges and Opportunities for Policy Makers,2023
https://doi.org/10.2139/ssrn.4371681,"Having its first commercial breakthrough with ChatGPT, in 2023 artificial intelligence (AI) has rung in the fourth industrial revolution. This article outlines the technicalities and economics underlying generative AI, and what they mean for competition in digital markets and for antitrust and regulatory lawyers working in this field. It concludes that due to the high barriers to entry at the (i) computing, (ii) data creation, and (iii) foundation modelling layers, the rise of AI is likely to make incumbent tech conglomerates even more powerful and less contestable, which will have to be considered in any future economic regulation of AI.",Competition (biology);Business;Law and economics;Political science;Economics;Biology;Ecology,"ChatGPT, Bard &amp;amp; Co.: An Introduction to AI for Competition and Regulatory Lawyers",2023
https://doi.org/10.54648/gplr2024007,"The People’s Republic of China (PRC) has moved ahead of other jurisdictions in regulating Artificial Intelligence (AI) by enacting a set of regulations aimed at addressing risks associated with AI.

 This article reviews three key AI specific regulations enacted in the PRC in 2022 and 2023. Together these regulations set out comprehensive obligations on the use of generative AI and algorithmic recommendation services in the PRC.

 The obligations include general requirements such as to comply with the law and principles of fairness as well as to protect against incorrect political direction and violation of social morality.

 There are also more specific obligations to achieve those objectives. These include, ensuring the regulator has visibility of certain algorithms by requiring the filing of information about the relevant algorithms. Further there are specific obligations on service providers such as to label training data appropriately, take measures to screen illegal or harmful content, verify the identities of users for some services, protect against fake news and label artificial intelligence generated content as synthetic. 

The regulations also provide a suite of rights for users, including the right not to be targeted on the basis of their personal characteristics by services using AI algorithms. 

The obligations under these regulations are significant and it will be interesting to see how the PRC’s regulatory approach impacts the AI industry in the PRC in contrast to other jurisdictions where regulators are still looking at how to regulate AI or taking a different approach.",China;Business;Political science;Law,Report: China’s New AI Regulations,2024
https://doi.org/10.1145/3670691,"In response to diverse perspectives on artificial general intelligence (AGI), ranging from potential safety and ethical concerns to more extreme views about the threats it poses to humanity, this research presents a generic method to gauge the reasoning capabilities of artificial intelligence (AI) models as a foundational step in evaluating safety measures. Recognizing that AI reasoning measures cannot be wholly automated, due to factors such as cultural complexity, we conducted an extensive examination of five commercial generative pre-trained transformers (GPTs), focusing on their comprehension and interpretation of culturally intricate contexts. Utilizing our novel “Reasoning and Value Alignment Test,” we assessed the GPT models’ ability to reason in complex situations and grasp local cultural subtleties. Our findings have indicated that, although the models have exhibited high levels of human-like reasoning, significant limitations remained, especially concerning the interpretation of cultural contexts. This article also explored potential applications and use-cases of our Test, underlining its significance in AI training, ethics compliance, sensitivity auditing, and AI-driven cultural consultation. We concluded by emphasizing its broader implications in the AGI domain, highlighting the necessity for interdisciplinary approaches, wider accessibility to various GPT models, and a profound understanding of the interplay between GPT reasoning and cultural sensitivity.",Visual reasoning;Value (mathematics);Qualitative reasoning,A Reasoning and Value Alignment Test to Assess Advanced GPT Reasoning,2024
https://doi.org/10.7821/naer.2023.1.1231,"Abstract In the context of Artificial Intelligence, Generative Adversarial Nets (GANs) allow the creation and reproduction of artificial data from real datasets. The aims of this work are to seek to verify the equivalence of synthetic data with real data and to verify the possibilities of GAN in educational research. The research methodology begins with the creation of a survey that collects data related to the self-perceptions of university teachers regarding their digital competence and technological-pedagogical knowledge of the content (TPACK model). Once the original dataset is generated, twenty-nine different synthetic samples are created (with an increasing N) using the COPULA-GAN procedure. Finally, a two-stage cluster analysis is applied to verify the interchangeability of the synthetic samples with the original, in addition to extracting descriptive data of the distribution characteristics, thereby checking the similarity of the qualitative results. In the results, qualitatively very similar cluster structures have been obtained in the 150 tests carried out, with a clear tendency to identify three types of teaching profiles, based on their level of technical-pedagogical knowledge of the content. It is concluded that the use of synthetic samples is an interesting way of improving data quality, both for security and anonymization and for increasing sample sizes.",Computer science;Generative grammar;Competence (human resources);Artificial intelligence;Data science;Psychology;Social psychology,Use of Generative Adversarial Networks (GANs) in Educational Technology Research,2023
https://doi.org/10.1002/joe.22238,"In light of the rise of generative AI and recent debates about the socio-political implications of large-language models, chatbots, and the like, this paper analyzes the E.U.’s Artificial Intelligence Act (AIA), the world's first comprehensive attempt by a government body to address and mitigate the potentially negative impacts of AI technologies. The paper critically analyzes the AIA from a business and computer ethics point of view—a perspective currently lacking in the academic (e.g., GBOE-related) literature. It evaluates, in particular, the AIA's strengths and weaknesses and proposes reform measures that could help to strengthen the AIA. Among the AIA's strengths are its legally binding character, extra-territoriality, ability to address data quality and discrimination risks, and institutional innovations such as the AI Board and publicly accessible logs and database for AI systems. Among its main weaknesses are its lack of effective enforcement, oversight, and control, absence of procedural rights and remedy mechanisms, inadequate worker protection, institutional ambiguities, insufficient funding and staffing, and inadequate consideration of sustainability issues. Reform suggestions include establishing independent conformity assessment procedures, strengthening democratic accountability and judicial oversight, introducing redress and complaint mechanisms, ensuring the participation and inclusion of workers, guaranteeing political independence of the AI Board, providing enhanced funding and staffing of market surveillance authorities, and mandating “green AI.”",Staffing;Redress;Strengths and weaknesses,Mitigating the adverse effects of AI with the European Union's artificial intelligence act: Hype or hope?,2023
https://doi.org/10.15282/ijleal.v14i1.10432,"The integration of artificial intelligence (AI) in schools presents significant challenges and risks requiring responsible and ethical management. Despite warnings from tech leaders, major corporations push AI adoption in schools, leading to privacy violations, biased algorithms and curricular misinformation. Generative AI, though enhancing resources, risks disseminating false information. Biased AI models perpetuate inequalities, especially for marginalized groups. The financial burdens of AI implementation worsen budget constraints, and AI-driven surveillance raises privacy concerns. Governance must prioritize ethics and student rights, establishing transparent frameworks to prevent commercial interests from overshadowing educational goals. This editorial suggests halting AI adoption until comprehensive legislation safeguards against risks. Stakeholders should prioritize responsible AI development, stressing transparency and accountability. Collaboration between AI developers and educators is essential to ensuring AI serves students and society responsibly.",Do no harm,Does Artificial Intelligence Cause More Harm than Good in Schools?,2024
https://doi.org/10.1097/coc.0000000000001050,"Abstract
Objectives: 
Artificial intelligence (AI) chatbots are a new, publicly available tool for patients to access health care–related information with unknown reliability related to cancer-related questions. This study assesses the quality of responses to common questions for patients with cancer.

Methods: 
From February to March 2023, we queried chat generative pretrained transformer (ChatGPT) from OpenAI and Bing AI from Microsoft questions from the American Cancer Society’s recommended “Questions to Ask About Your Cancer” customized for all stages of breast, colon, lung, and prostate cancer. Questions were, in addition, grouped by type (prognosis, treatment, or miscellaneous). The quality of AI chatbot responses was assessed by an expert panel using the validated DISCERN criteria.

Results: 
Of the 117 questions presented to ChatGPT and Bing, the average score for all questions were 3.9 and 3.2, respectively (P < 0.001) and the overall DISCERN scores were 4.1 and 4.4, respectively. By disease site, the average score for ChatGPT and Bing, respectively, were 3.9 and 3.6 for prostate cancer (P = 0.02), 3.7 and 3.3 for lung cancer (P < 0.001), 4.1 and 2.9 for breast cancer (P < 0.001), and 3.8 and 3.0 for colorectal cancer (P < 0.001). By type of question, the average score for ChatGPT and Bing, respectively, were 3.6 and 3.4 for prognostic questions (P = 0.12), 3.9 and 3.1 for treatment questions (P < 0.001), and 4.2 and 3.3 for miscellaneous questions (P = 0.001). For 3 responses (3%) by ChatGPT and 18 responses (15%) by Bing, at least one panelist rated them as having serious or extensive shortcomings.

Conclusions: 
AI chatbots provide multiple opportunities for innovating health care. This analysis suggests a critical need, particularly around cancer prognostication, for continual refinement to limit misleading counseling, confusion, and emotional distress to patients and families.",Medicine;Colorectal cancer;Breast cancer;Lung cancer;Cancer;Prostate cancer;Internal medicine;Family medicine;Oncology,Physician Assessment of ChatGPT and Bing Answers to American Cancer Society’s Questions to Ask About Your Cancer,2023
https://doi.org/10.1145/3657054.3657125,"The landscape of artificial intelligence (AI) has experienced a monumental shift with the emerging of Generative AI (GenAI), which has demonstrated to be a transformative tool across diverse sectors. GenAI outputs can span various digital formats, including text, images, videos, and audio, generating particular interest in the public sector. The growing interest of governments in integrating GenAI technologies in public sector operations is marked by the creation of emerging governance instruments and the formulation of soft laws, like standards, principles, and guidelines. This study aims to delve into the intricacies and potential risks associated with the deployment of GenAI within government. Through a qualitative content analysis, the research meticulously examines GenAI usage guidelines issued by Australia, Canada, New Zealand, the United Kingdom, and South Korea. The objective is to discern the risks acknowledged by these countries' soft laws and compare them with the risks identified by scholars in the field. The performed comparative analysis across countries suggest that the use of GenAI in the public sector raises common risks such as information leakage, data privacy, security, and concerns over public trust. By elucidating the varied risk perceptions across different national contexts, this study provides theoretical and practical implications related to the risks of GenAI within the public sector. Moreover, it sets a foundation for future research and policy development, ensuring that generative AI is used as a force for good in public governance.",Transformative Learning,Comparative Analysis of Generative AI Risks in the Public Sector,2024
https://doi.org/10.3390/su16114451,"In an era marked by unprecedented global challenges, including environmental degradation, social inequalities, and the rapid evolution of technology, the need for innovative educational approaches is critical. This conceptual paper explores the intersection of sustainability, creativity, and technology for education, focusing on artificial intelligence (AI) as an example. We propose a framework that synthesizes sustainability principles and creative pedagogies, detailing its components to guide the integration of AI into sustainability education. The paper illustrates how blending creative pedagogies with the notion of sustainability as a frame of mind offers a framework that allows teachers to support creative learning and problem solving, with and through technology. Using the example of AI technology, we illustrate the potential benefits and inherent challenges of integrating new technologies into education. Generative AI is a cogent example, as it presents unique opportunities for personalizing learning and engaging students in creative problem solving around sustainability issues. However, it also introduces significant environmental and ethical concerns to navigate. Exploring the balance between technological innovation and sustainability imperatives, this paper outlines a framework for incorporating technology into education that promotes environmental care with creative exploration. Through a synthesis of sustainability principles and creative pedagogies, we highlight the benefits and challenges of using AI in education, offering strategic insights to leverage technology for a sustainable and just future.",Mindset,"Creative Learning for Sustainability in a World of AI: Action, Mindset, Values",2024
